{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9b568712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Grand Slam matches…\n",
      "Loading and stacking ranking doubles files…\n",
      "Attaching ranks…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aldi\\AppData\\Local\\Temp\\ipykernel_13524\\684776947.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out_df = pd.concat(out_chunks, ignore_index=True) if out_chunks else pd.DataFrame({\"match_id\": []})\n",
      "C:\\Users\\aldi\\AppData\\Local\\Temp\\ipykernel_13524\\684776947.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out_df = pd.concat(out_chunks, ignore_index=True) if out_chunks else pd.DataFrame({\"match_id\": []})\n",
      "C:\\Users\\aldi\\AppData\\Local\\Temp\\ipykernel_13524\\684776947.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out_df = pd.concat(out_chunks, ignore_index=True) if out_chunks else pd.DataFrame({\"match_id\": []})\n",
      "C:\\Users\\aldi\\AppData\\Local\\Temp\\ipykernel_13524\\684776947.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out_df = pd.concat(out_chunks, ignore_index=True) if out_chunks else pd.DataFrame({\"match_id\": []})\n",
      "C:\\Users\\aldi\\AppData\\Local\\Temp\\ipykernel_13524\\684776947.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out_df = pd.concat(out_chunks, ignore_index=True) if out_chunks else pd.DataFrame({\"match_id\": []})\n",
      "C:\\Users\\aldi\\AppData\\Local\\Temp\\ipykernel_13524\\684776947.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out_df = pd.concat(out_chunks, ignore_index=True) if out_chunks else pd.DataFrame({\"match_id\": []})\n",
      "C:\\Users\\aldi\\AppData\\Local\\Temp\\ipykernel_13524\\684776947.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out_df = pd.concat(out_chunks, ignore_index=True) if out_chunks else pd.DataFrame({\"match_id\": []})\n",
      "C:\\Users\\aldi\\AppData\\Local\\Temp\\ipykernel_13524\\684776947.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  out_df = pd.concat(out_chunks, ignore_index=True) if out_chunks else pd.DataFrame({\"match_id\": []})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote unmatched names list to: C:/Users/aldi/Documents/GitHub/tennis-homophily/data/atp/_unmatched_names_for_manual_mapping.txt\n",
      "Saving to C:/Users/aldi/Documents/GitHub/tennis-homophily/data/atp/grand_slam_matches_with_ranks_2018_2023.xlsx …\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from getpass import getuser\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG\n",
    "# -----------------------------\n",
    "USER = getuser()\n",
    "INPUT_DIR = f\"C:/Users/{USER}/Documents/GitHub/tennis-homophily/data/atp/\"\n",
    "GRAND_SLAM_FILE = os.path.join(INPUT_DIR, \"grand_slam_matches_2018_2023.xlsx\")\n",
    "OUTPUT_FILE = os.path.join(INPUT_DIR, \"grand_slam_matches_with_ranks_2018_2023.xlsx\")\n",
    "\n",
    "# Ranking file columns (edit if your headers differ)\n",
    "RANKING_PLAYER_COL = \"Player\"     # e.g. \"J. Cabal\"\n",
    "RANKING_DATE_COL   = \"DateWeek\"   # Monday of the ATP ranking week\n",
    "RANKING_VALUE_COL  = \"Rank\"       # numeric rank\n",
    "\n",
    "# Keep ALL ranking columns by default. If you want a subset, list it here.\n",
    "PROFILE_KEEP_WHITELIST: list[str] = []  # e.g. [\"W-L YTD\",\"W-L Career\",\"Titles YTD\",\"Titles Career\",\"DOB\",\"Country\",\"Plays\",\"Coach\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Name normalization\n",
    "# -----------------------------\n",
    "def strip_parentheses_payload(name: str) -> str:\n",
    "    if pd.isna(name):\n",
    "        return name\n",
    "    return re.sub(r\"\\([^)]*\\)\", \"\", str(name)).strip()\n",
    "\n",
    "def normalize_spaces(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", str(s)).strip()\n",
    "\n",
    "def to_initial_last_key_from_full(fullname: str) -> str | None:\n",
    "    \"\"\"'Juan Sebastian Cabal(11)' -> 'j cabal'\"\"\"\n",
    "    if pd.isna(fullname) or not str(fullname).strip():\n",
    "        return None\n",
    "    s = normalize_spaces(strip_parentheses_payload(fullname)).replace(\"-\", \" \")\n",
    "    parts = [p for p in s.split(\" \") if p]\n",
    "    if not parts:\n",
    "        return None\n",
    "    return f\"{parts[0][0].lower()} {parts[-1].lower()}\"\n",
    "\n",
    "def clean_ranking_player_raw(raw: str) -> str | None:\n",
    "    if pd.isna(raw) or not str(raw).strip():\n",
    "        return None\n",
    "    s = re.sub(r\"^[^A-Za-z]+\", \"\", str(raw))  # drop prefixes like \"-1\"\n",
    "    return normalize_spaces(s)\n",
    "\n",
    "def to_initial_last_key_from_initialdot(name: str) -> str | None:\n",
    "    \"\"\"'J. Cabal'/'J Cabal' -> 'j cabal'\"\"\"\n",
    "    if pd.isna(name) or not str(name).strip():\n",
    "        return None\n",
    "    s = normalize_spaces(name).replace(\".\", \"\").replace(\"-\", \" \")\n",
    "    parts = [p for p in s.split(\" \") if p]\n",
    "    if not parts:\n",
    "        return None\n",
    "    if len(parts) == 1:\n",
    "        return f\"{parts[0][0].lower()} {parts[0].lower()}\"\n",
    "    return f\"{parts[0][0].lower()} {parts[-1].lower()}\"\n",
    "\n",
    "# Manual disambiguations (fill as needed)\n",
    "MANUAL_KEY_MAP: dict[str, str] = {\n",
    "    # \"j smith\": \"j smith\",\n",
    "}\n",
    "\n",
    "def gs_key(name: str) -> str | None:\n",
    "    key = to_initial_last_key_from_full(name)\n",
    "    return MANUAL_KEY_MAP.get(key, key)\n",
    "\n",
    "def rk_key(name: str) -> str | None:\n",
    "    return to_initial_last_key_from_initialdot(clean_ranking_player_raw(name))\n",
    "\n",
    "# -----------------------------\n",
    "# Profile-field cleaning (ranking files)\n",
    "# -----------------------------\n",
    "DOB_PAT = re.compile(r\"\\((\\d{4})[/-](\\d{1,2})[/-](\\d{1,2})\\)\")  # (YYYY/MM/DD) or (YYYY-MM-DD)\n",
    "KG_PAT  = re.compile(r\"\\((\\d{2,3})\\s*kg\\)\", flags=re.I)\n",
    "CM_PAT  = re.compile(r\"\\((\\d{2,3})\\s*cm\\)\", flags=re.I)\n",
    "\n",
    "def extract_dob_from_age(age_val):\n",
    "    # \"32 (1993/07/04)\" -> \"1993-07-04\"\n",
    "    if pd.isna(age_val):\n",
    "        return None\n",
    "    m = DOB_PAT.search(str(age_val))\n",
    "    if not m:\n",
    "        return None\n",
    "    y, mo, d = m.groups()\n",
    "    try:\n",
    "        return pd.Timestamp(int(y), int(mo), int(d)).strftime(\"%Y-%m-%d\")\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def extract_kg(weight_val):\n",
    "    # \"183 lbs (83kg)\" or \"83 kg\" -> 83.0\n",
    "    if pd.isna(weight_val):\n",
    "        return None\n",
    "    s = str(weight_val)\n",
    "    m = KG_PAT.search(s) or re.search(r\"(\\d{2,3})\\s*kg\", s, flags=re.I)\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "def extract_cm(height_val):\n",
    "    # \"6'3\\\" (191cm)\" or \"191 cm\" -> 191.0\n",
    "    if pd.isna(height_val):\n",
    "        return None\n",
    "    s = str(height_val)\n",
    "    m = CM_PAT.search(s) or re.search(r\"(\\d{2,3})\\s*cm\", s, flags=re.I)\n",
    "    return float(m.group(1)) if m else None\n",
    "\n",
    "# -----------------------------\n",
    "# Parse tournament start (from gs 'date')\n",
    "# -----------------------------\n",
    "def parse_tournament_start(date_str: str, fallback_year: int) -> pd.Timestamp:\n",
    "    # e.g., \"15-28 Jan, 2018\" -> 2018-01-15\n",
    "    if pd.isna(date_str):\n",
    "        return pd.Timestamp(year=fallback_year, month=1, day=1)\n",
    "    s = str(date_str)\n",
    "    m = re.search(r\"(\\d{1,2})\\s*[-–]\\s*(\\d{1,2})?\\s*([A-Za-z]{3,})[^0-9]*([12][0-9]{3})\", s)\n",
    "    if m:\n",
    "        day1 = int(m.group(1))\n",
    "        mon  = m.group(3)\n",
    "        year = int(m.group(4))\n",
    "        try:\n",
    "            month_num = datetime.strptime(mon[:3], \"%b\").month\n",
    "        except ValueError:\n",
    "            month_num = 1\n",
    "        return pd.Timestamp(year=year, month=month_num, day=day1)\n",
    "    for fmt in (\"%Y-%m-%d\", \"%d/%m/%Y\", \"%d-%m-%Y\"):\n",
    "        try:\n",
    "            return pd.to_datetime(s, format=fmt)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return pd.Timestamp(year=fallback_year, month=1, day=1)\n",
    "\n",
    "# -----------------------------\n",
    "# Load Grand Slam\n",
    "# -----------------------------\n",
    "print(\"Loading Grand Slam matches…\")\n",
    "if not os.path.exists(GRAND_SLAM_FILE):\n",
    "    raise FileNotFoundError(f\"Grand Slam file not found: {GRAND_SLAM_FILE}\")\n",
    "\n",
    "gs = pd.read_excel(GRAND_SLAM_FILE)\n",
    "if \"year\" not in gs.columns:\n",
    "    if \"date\" in gs.columns:\n",
    "        gs[\"year\"] = gs[\"date\"].astype(str).str.extract(r\"([12][0-9]{3})\").astype(int)\n",
    "    else:\n",
    "        raise KeyError(\"Grand Slam file needs a 'year' or a parsable 'date' column.\")\n",
    "\n",
    "if \"match_date\" not in gs.columns:\n",
    "    gs[\"match_date\"] = gs.apply(lambda r: parse_tournament_start(r.get(\"date\", None), int(r[\"year\"])), axis=1)\n",
    "gs = gs.reset_index().rename(columns={\"index\": \"match_id\"})\n",
    "\n",
    "# -----------------------------\n",
    "# Load & clean ranking files\n",
    "# -----------------------------\n",
    "print(\"Loading and stacking ranking doubles files…\")\n",
    "ranking_files = sorted(glob.glob(os.path.join(INPUT_DIR, \"ranking_doubles_*.xlsx\")))\n",
    "if not ranking_files:\n",
    "    raise FileNotFoundError(\"No ranking_doubles_*.xlsx files found in INPUT_DIR\")\n",
    "\n",
    "rk_rows = []\n",
    "for fp in ranking_files:\n",
    "    df = pd.read_excel(fp)\n",
    "\n",
    "    # Required columns check\n",
    "    missing = [c for c in [RANKING_PLAYER_COL, RANKING_DATE_COL, RANKING_VALUE_COL] if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing columns {missing} in ranking file {os.path.basename(fp)}\")\n",
    "\n",
    "    # Keep ALL columns or whitelist subset\n",
    "    if PROFILE_KEEP_WHITELIST:\n",
    "        base_cols = {RANKING_PLAYER_COL, RANKING_DATE_COL, RANKING_VALUE_COL}\n",
    "        keep_cols = list(base_cols.union([c for c in PROFILE_KEEP_WHITELIST if c in df.columns]))\n",
    "        df = df[keep_cols].copy()\n",
    "    else:\n",
    "        df = df.copy()\n",
    "\n",
    "    # Field cleaning BEFORE keying\n",
    "    if \"Age\" in df.columns:\n",
    "        dob_from_age = df[\"Age\"].apply(extract_dob_from_age)\n",
    "        if \"DOB\" in df.columns:\n",
    "            df[\"DOB\"] = df[\"DOB\"].where(df[\"DOB\"].notna() & (df[\"DOB\"].astype(str).str.len() > 0), dob_from_age)\n",
    "        else:\n",
    "            df[\"DOB\"] = dob_from_age\n",
    "        df.drop(columns=[\"Age\"], inplace=True)\n",
    "\n",
    "    if \"Weight\" in df.columns:\n",
    "        df[\"WeightKg\"] = df[\"Weight\"].apply(extract_kg)\n",
    "        df.drop(columns=[\"Weight\"], inplace=True)\n",
    "\n",
    "    if \"Height\" in df.columns:\n",
    "        df[\"HeightCm\"] = df[\"Height\"].apply(extract_cm)\n",
    "        df.drop(columns=[\"Height\"], inplace=True)\n",
    "\n",
    "    # Date & key\n",
    "    df[RANKING_DATE_COL] = pd.to_datetime(df[RANKING_DATE_COL], errors=\"coerce\").dt.tz_localize(None)\n",
    "    df[\"player_key\"] = df[RANKING_PLAYER_COL].map(rk_key)\n",
    "    df = df.dropna(subset=[\"player_key\", RANKING_DATE_COL]).copy()\n",
    "\n",
    "    rk_rows.append(df)\n",
    "\n",
    "rk = pd.concat(rk_rows, ignore_index=True)\n",
    "rk = rk.sort_values([\"player_key\", RANKING_DATE_COL], kind=\"mergesort\").reset_index(drop=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Per-player as-of attach (keep ALL ranking columns, prefixed)\n",
    "# -----------------------------\n",
    "def attach_rank_for(gs_df: pd.DataFrame, player_col: str, out_prefix: str) -> pd.DataFrame:\n",
    "    tmp = gs_df[[\"match_id\", \"match_date\", player_col]].copy()\n",
    "    tmp[\"match_date\"] = pd.to_datetime(tmp[\"match_date\"], errors=\"coerce\").dt.tz_localize(None)\n",
    "    tmp[\"player_key\"] = tmp[player_col].map(gs_key)\n",
    "    tmp = tmp.dropna(subset=[\"player_key\", \"match_date\"]).copy()\n",
    "\n",
    "    # bring all ranking columns except helper key\n",
    "    rk_cols = [c for c in rk.columns if c not in (\"player_key\",)]\n",
    "\n",
    "    out_chunks = []\n",
    "    for pkey, g in tmp.groupby(\"player_key\", sort=False):\n",
    "        g = g.sort_values(\"match_date\").reset_index(drop=True)\n",
    "        r = rk.loc[rk[\"player_key\"] == pkey, rk_cols].copy()\n",
    "\n",
    "        if r.empty:\n",
    "            for c in rk_cols:\n",
    "                g[f\"{out_prefix}{c}\"] = pd.NA\n",
    "        else:\n",
    "            r[RANKING_DATE_COL] = pd.to_datetime(r[RANKING_DATE_COL], errors=\"coerce\").dt.tz_localize(None)\n",
    "            r = r.dropna(subset=[RANKING_DATE_COL]).sort_values(RANKING_DATE_COL).reset_index(drop=True)\n",
    "\n",
    "            merged = pd.merge_asof(\n",
    "                g, r,\n",
    "                left_on=\"match_date\",\n",
    "                right_on=RANKING_DATE_COL,\n",
    "                direction=\"backward\",\n",
    "                allow_exact_matches=True,\n",
    "            )\n",
    "            for c in rk_cols:\n",
    "                g[f\"{out_prefix}{c}\"] = merged[c].values\n",
    "\n",
    "        out_chunks.append(g[[\"match_id\"] + [f\"{out_prefix}{c}\" for c in rk_cols]])\n",
    "\n",
    "    out_df = pd.concat(out_chunks, ignore_index=True) if out_chunks else pd.DataFrame({\"match_id\": []})\n",
    "    return gs_df.merge(out_df, on=\"match_id\", how=\"left\")\n",
    "\n",
    "# -----------------------------\n",
    "# Attach for all four players\n",
    "# -----------------------------\n",
    "print(\"Attaching ranks…\")\n",
    "roles = [\n",
    "    (\"winners_p1\", \"winners_p1_\"),\n",
    "    (\"winners_p2\", \"winners_p2_\"),\n",
    "    (\"losers_p1\",  \"losers_p1_\"),\n",
    "    (\"losers_p2\",  \"losers_p2_\"),\n",
    "]\n",
    "for col, prefix in roles:\n",
    "    if col not in gs.columns:\n",
    "        raise KeyError(f\"Column '{col}' is missing from Grand Slam file.\")\n",
    "    gs = attach_rank_for(gs, col, prefix)\n",
    "\n",
    "# -----------------------------\n",
    "# Unmatched names helper list\n",
    "# -----------------------------\n",
    "problem_names = set()\n",
    "for col, prefix in roles:\n",
    "    rank_col = f\"{prefix}{RANKING_VALUE_COL}\"   # e.g. winners_p1_Rank\n",
    "    if rank_col in gs.columns:\n",
    "        mask = gs[rank_col].isna()\n",
    "        problem_names.update(gs.loc[mask, col].dropna().unique().tolist())\n",
    "    else:\n",
    "        date_col = f\"{prefix}{RANKING_DATE_COL}\"\n",
    "        if date_col in gs.columns:\n",
    "            mask = gs[date_col].isna()\n",
    "            problem_names.update(gs.loc[mask, col].dropna().unique().tolist())\n",
    "\n",
    "if problem_names:\n",
    "    problems_path = os.path.join(INPUT_DIR, \"_unmatched_names_for_manual_mapping.txt\")\n",
    "    with open(problems_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for n in sorted(problem_names):\n",
    "            f.write(str(n) + \"\\n\")\n",
    "    print(f\"Wrote unmatched names list to: {problems_path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Save\n",
    "# -----------------------------\n",
    "print(f\"Saving to {OUTPUT_FILE} …\")\n",
    "with pd.ExcelWriter(OUTPUT_FILE, engine=\"xlsxwriter\") as xlw:\n",
    "    gs.to_excel(xlw, index=False, sheet_name=\"matches_with_ranks\")\n",
    "\n",
    "print(\"Done.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
