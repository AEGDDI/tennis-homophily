{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"aldi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the ChromeDriver\n",
    "chrome_driver_path = f\"C:/Users/{user}/Downloads/chromedriver.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = f\"C:/Users/{user}/Documents/GitHub/tennis-homophily/data/atp\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run functions.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Dates:\n",
      "2023-12-04\n",
      "2023-11-27\n",
      "2023-10-30\n",
      "2023-09-25\n",
      "2023-08-28\n",
      "2023-07-31\n",
      "2023-06-26\n",
      "2023-05-29\n",
      "2023-04-24\n",
      "2023-03-20\n",
      "2023-02-27\n",
      "2023-01-30\n",
      "2022-12-26\n",
      "2022-11-28\n",
      "2022-10-31\n",
      "2022-09-26\n",
      "2022-08-29\n",
      "2022-07-25\n",
      "2022-06-27\n",
      "2022-05-23\n",
      "2022-04-25\n",
      "2022-03-21\n",
      "2022-02-28\n",
      "2022-01-31\n",
      "2021-12-27\n",
      "2021-11-29\n",
      "2021-10-25\n",
      "2021-09-27\n",
      "2021-08-30\n",
      "2021-07-26\n",
      "2021-06-28\n",
      "2021-05-31\n",
      "2021-04-26\n",
      "2021-03-22\n",
      "2021-02-22\n",
      "2021-01-25\n",
      "Rank Range: 1-5000, Rank Date: 2023-12-04 - Data saved to C:/Users/aldi/Documents/GitHub/tennis-homophily/data/atp\\rankings_data_2023-12-04.csv\n",
      "\n",
      "Rank Range: 1-5000, Rank Date: 2023-11-27 - Data saved to C:/Users/aldi/Documents/GitHub/tennis-homophily/data/atp\\rankings_data_2023-11-27.csv\n",
      "\n",
      "Rank Range: 1-5000, Rank Date: 2023-10-30 - Data saved to C:/Users/aldi/Documents/GitHub/tennis-homophily/data/atp\\rankings_data_2023-10-30.csv\n",
      "\n",
      "Rank Range: 1-5000, Rank Date: 2023-09-25 - Data saved to C:/Users/aldi/Documents/GitHub/tennis-homophily/data/atp\\rankings_data_2023-09-25.csv\n",
      "\n",
      "Rank Range: 1-5000, Rank Date: 2023-08-28 - Data saved to C:/Users/aldi/Documents/GitHub/tennis-homophily/data/atp\\rankings_data_2023-08-28.csv\n",
      "\n",
      "Rank Range: 1-5000, Rank Date: 2023-07-31 - Data saved to C:/Users/aldi/Documents/GitHub/tennis-homophily/data/atp\\rankings_data_2023-07-31.csv\n",
      "\n",
      "Rank Range: 1-5000, Rank Date: 2023-06-26 - Data saved to C:/Users/aldi/Documents/GitHub/tennis-homophily/data/atp\\rankings_data_2023-06-26.csv\n",
      "\n",
      "Rank Range: 1-5000, Rank Date: 2023-05-29 - Data saved to C:/Users/aldi/Documents/GitHub/tennis-homophily/data/atp\\rankings_data_2023-05-29.csv\n",
      "\n",
      "Rank Range: 1-5000, Rank Date: 2023-04-24 - Data saved to C:/Users/aldi/Documents/GitHub/tennis-homophily/data/atp\\rankings_data_2023-04-24.csv\n",
      "\n",
      "Rank Range: 1-5000, Rank Date: 2023-03-20 - Data saved to C:/Users/aldi/Documents/GitHub/tennis-homophily/data/atp\\rankings_data_2023-03-20.csv\n",
      "\n",
      "Rank Range: 1-5000, Rank Date: 2023-02-27 - Data saved to C:/Users/aldi/Documents/GitHub/tennis-homophily/data/atp\\rankings_data_2023-02-27.csv\n",
      "\n",
      "Rank Range: 1-5000, Rank Date: 2023-01-30 - Data saved to C:/Users/aldi/Documents/GitHub/tennis-homophily/data/atp\\rankings_data_2023-01-30.csv\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-d5857b427ac2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m             \u001b[1;31m# Open the URL and wait for the content to load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[0mdriver2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplete_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Wait for 5 seconds for dynamic content to load, adjust as needed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m             \u001b[1;31m# Get the page source and create BeautifulSoup object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use the Service object for ChromeDriver\n",
    "service = Service(chrome_driver_path)\n",
    "\n",
    "try:\n",
    "    # Initialize the list of dates to be scraped\n",
    "    date_values = []\n",
    "\n",
    "    # Open the first part to get the list of dates\n",
    "    with webdriver.Chrome(service=service) as driver1:\n",
    "        # Open the URL and wait for the content to load\n",
    "        driver1.get(\"https://www.atptour.com/en/rankings/doubles?rankRange=1-5000\")\n",
    "        driver1.maximize_window()\n",
    "        WebDriverWait(driver1, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"mega-table\")))\n",
    "\n",
    "        # Find the date dropdown menu and retrieve the date options\n",
    "        date_dropdown_ul = WebDriverWait(driver1, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"ul[data-value='rankDate']\"))\n",
    "        )\n",
    "        date_options = date_dropdown_ul.find_elements(By.TAG_NAME, \"li\")\n",
    "        \n",
    "        # Extract date values from the dropdown and store them in a list\n",
    "#         date_values = [option.get_attribute(\"data-value\") for option in date_options]\n",
    "\n",
    "        # Extract date values from the dropdown and store them in a list - applying year filter\n",
    "        for option in date_options:\n",
    "            date_value = option.get_attribute(\"data-value\")\n",
    "            if date_value is None:\n",
    "                continue\n",
    "            try:\n",
    "                year = int(date_value[:4])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if year > 2022:\n",
    "                date_values.append(date_value)\n",
    "                \n",
    "    # Filter the dates to keep only the highest number for each pair of year and month\n",
    "    filtered_dates = get_highest_dates(date_values)\n",
    "\n",
    "\n",
    "    print(\"List of Dates:\")\n",
    "    for date in filtered_dates :\n",
    "        print(date)\n",
    "\n",
    "    # Now, loop through different dates and scrape the data\n",
    "    for date in filtered_dates :\n",
    "        # Form the complete URL with the selected date\n",
    "        complete_url = f\"https://www.atptour.com/en/rankings/doubles?rankRange=1-5000&rankDate={date}\"\n",
    "\n",
    "        # Open a new WebDriver for each date\n",
    "        with webdriver.Chrome(service=service) as driver2:\n",
    "            # Open the URL and wait for the content to load\n",
    "            driver2.get(complete_url)\n",
    "            time.sleep(5)  # Wait for 5 seconds for dynamic content to load, adjust as needed\n",
    "\n",
    "            # Get the page source and create BeautifulSoup object\n",
    "            page_source = driver2.page_source\n",
    "            soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "            # Continue with extracting data as before\n",
    "            rankings_table = soup.find(\"table\", {\"class\": \"mega-table\"})\n",
    "            rows = rankings_table.find_all(\"tr\")[1:]  # Skip the header row\n",
    "\n",
    "            # Create a CSV file for each date and save the data\n",
    "            filename = os.path.join(output_folder, f\"rankings_data_{date}.csv\")\n",
    "            with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "                writer = csv.writer(file)\n",
    "                writer.writerow([\"Rank\", \"Player\", \"Age\", \"Points\", \"Tournaments Played\"])\n",
    "\n",
    "                for row in rows:\n",
    "                    rank = row.find(\"td\", {\"class\": \"rank-cell\"}).text.strip()\n",
    "                    player_name = row.find(\"td\", {\"class\": \"player-cell\"}).text.strip()\n",
    "                    age = row.find(\"td\", {\"class\": \"age-cell\"}).text.strip()\n",
    "                    points = row.find(\"td\", {\"class\": \"points-cell\"}).text.strip()\n",
    "                    tournaments_played = row.find(\"td\", {\"class\": \"tourn-cell\"}).text.strip()\n",
    "\n",
    "                    writer.writerow([rank, player_name, age, points, tournaments_played])\n",
    "\n",
    "            print(f\"Rank Range: 1-5000, Rank Date: {date} - Data saved to {filename}\")\n",
    "            print()\n",
    "\n",
    "finally:\n",
    "    # Don't forget to stop the service once you are done.\n",
    "    service.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ALESSANDRO\\AppData\\Local\\Temp\\ipykernel_13144\\3261235652.py:8: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path=chrome_driver_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year Turned Pro: 2008\n",
      "Weight (kg): 78kg\n",
      "Height (cm): 180cm\n",
      "City of Birthplace: Zevenaar\n",
      "Country of Birthplace: Netherlands\n",
      "Hand: Right-Handed\n",
      "Backhand: Two-Handed Backhand\n",
      "Coaches:\n",
      "Coach 1: Rob Morgan\n",
      "Coach 2: Mariusz Fyrstenberg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize Chrome WebDriver\n",
    "driver = webdriver.Chrome(executable_path=chrome_driver_path)\n",
    "\n",
    "try:\n",
    "    url = \"https://www.atptour.com/en/players/wesley-koolhof/kc41/overview\"\n",
    "\n",
    "    # Open the URL using Selenium\n",
    "    driver.get(url)\n",
    "\n",
    "    # Get the page source after the dynamic content has loaded\n",
    "    page_source = driver.page_source\n",
    "\n",
    "    # Parse the HTML content of the page using BeautifulSoup\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "    # Find the player profile hero table\n",
    "    player_hero_table = soup.find(\"div\", class_=\"player-profile-hero-table\")\n",
    "\n",
    "    # Find the div with class \"table-big-label\" for \"Turned Pro\"\n",
    "    turned_pro_label_div = player_hero_table.find(\"div\", class_=\"table-big-label\", text=\"Turned Pro\")\n",
    "\n",
    "    # Extract the \"Turned Pro\" information\n",
    "    year_pro_div = turned_pro_label_div.find_next_sibling(\"div\", class_=\"table-big-value\")\n",
    "    year_pro = year_pro_div.get_text(strip=True)\n",
    "    \n",
    "    # Find the span with class \"table-weight-kg-wrapper\" for weight in kg\n",
    "    weight_span = player_hero_table.select_one(\"span.table-weight-kg-wrapper\")\n",
    "\n",
    "    # Extract weight in kg information\n",
    "    weight_kg = \"\"\n",
    "    if weight_span:\n",
    "        weight_kg = weight_span.get_text(strip=True).strip(\"()\")\n",
    "\n",
    "    # Find the span with class \"table-height-cm-wrapper\" for height in cm\n",
    "    height_span = player_hero_table.select_one(\"span.table-height-cm-wrapper\")\n",
    "\n",
    "    # Extract height in cm information\n",
    "    height_cm = \"\"\n",
    "    if height_span:\n",
    "        height_cm = height_span.get_text(strip=True).strip(\"()\")\n",
    "\n",
    "    # Find the player birthplace div with class \"table-value\"\n",
    "    birthplace_div = soup.find(\"div\", class_=\"table-value\")\n",
    "\n",
    "    # Extract birthplace information\n",
    "    birthplace = birthplace_div.get_text(strip=True)\n",
    "\n",
    "    # Split the birthplace into city and country\n",
    "    city_birthplace, country_birthplace = birthplace.split(\",\")\n",
    "\n",
    "    # Remove any leading or trailing whitespaces from city and country\n",
    "    city_birthplace = city_birthplace.strip()\n",
    "    country_birthplace = country_birthplace.strip()\n",
    "\n",
    "    # Find the second \"div\" with class \"table-value\" for hand and backhand\n",
    "    hand_backhand_div = player_hero_table.find_all(\"div\", class_=\"table-value\")[1]\n",
    "    \n",
    "    # Extract hand and backhand information if available\n",
    "    hand, backhand = \"\", \"\"\n",
    "    if hand_backhand_div:\n",
    "        hand, backhand = [item.strip() for item in hand_backhand_div.get_text(strip=True).split(\",\")]\n",
    "\n",
    "    # Find the third \"div\" with class \"table-value\" for coach information\n",
    "    coach_div = player_hero_table.find_all(\"div\", class_=\"table-value\")[2]\n",
    "\n",
    "    # Extract coach information if available\n",
    "    coaches = coach_div.get_text(strip=True).split(\", \")\n",
    "\n",
    "    # Print the extracted information\n",
    "    print(f\"Year Turned Pro: {year_pro}\")\n",
    "    print(f\"Weight (kg): {weight_kg}\")\n",
    "    print(f\"Height (cm): {height_cm}\")\n",
    "    print(f\"City of Birthplace: {city_birthplace}\")\n",
    "    print(f\"Country of Birthplace: {country_birthplace}\")\n",
    "    print(f\"Hand: {hand}\")\n",
    "    print(f\"Backhand: {backhand}\")\n",
    "    print(\"Coaches:\")\n",
    "    for idx, coach in enumerate(coaches, 1):\n",
    "        print(f\"Coach {idx}: {coach}\")\n",
    "\n",
    "finally:\n",
    "    # Don't forget to close the WebDriver once you are done.\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "combine the two chunks above in order to get all the information for each player in the rank table. (Test the first 10 players of the last 2023 tournament)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank Range: 1-5000, Rank Date: 2023-01-30 - Data saved to C:\\Users\\ALESSANDRO\\Documents\\GitHub\\tennis-homophily\\data\\atp\\rankings_data_2023-01-30.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Use the Service object for ChromeDriver\n",
    "service = Service(chrome_driver_path)\n",
    "\n",
    "try:\n",
    "    # Initialize the list of dates to be scraped\n",
    "    date_values = []\n",
    "\n",
    "    # Open the first part to get the list of dates\n",
    "    with webdriver.Chrome(service=service) as driver1:\n",
    "        # Open the URL and wait for the content to load\n",
    "        driver1.get(\"https://www.atptour.com/en/rankings/doubles?rankRange=1-5000\")\n",
    "        WebDriverWait(driver1, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"mega-table\")))\n",
    "\n",
    "        # Find the date dropdown menu and retrieve the date options\n",
    "        date_dropdown_ul = WebDriverWait(driver1, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"ul[data-value='rankDate']\"))\n",
    "        )\n",
    "        date_options = date_dropdown_ul.find_elements(By.TAG_NAME, \"li\")\n",
    "\n",
    "        # Extract date values from the dropdown and store them in a list - applying year filter\n",
    "        for option in date_options:\n",
    "            date_value = option.get_attribute(\"data-value\")\n",
    "            if date_value is None:\n",
    "                continue\n",
    "            try:\n",
    "                year = int(date_value[:4])\n",
    "            except ValueError:\n",
    "                continue\n",
    "            if year == 2023:\n",
    "                date_values.append(date_value)\n",
    "\n",
    "    # Filter the dates to keep only the highest number for each pair of year and month\n",
    "    filtered_dates = get_highest_dates(date_values)\n",
    "\n",
    "    # Use only one date for testing purposes\n",
    "    filtered_dates = [filtered_dates[-1]]\n",
    "\n",
    "    # Now, loop through different dates and scrape the data\n",
    "    for date in filtered_dates:\n",
    "        # Create a CSV file for each date and save the data\n",
    "        output_folder = \"C:\\\\Users\\\\ALESSANDRO\\\\Documents\\\\GitHub\\\\tennis-homophily\\\\data\\\\atp\"\n",
    "        filename = os.path.join(output_folder, f\"rankings_data_{date}.csv\")\n",
    "        with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Rank\", \"Player\", \"Age\", \"Points\", \"Tournaments Played\", \"Year Turned Pro\", \"Weight (kg)\", \"Height (cm)\", \"City of Birthplace\", \"Country of Birthplace\", \"Hand\", \"Backhand\", \"Coach1\", \"Coach2\"])\n",
    "\n",
    "            # Open a new WebDriver for each date\n",
    "            with webdriver.Chrome(service=service) as driver2:\n",
    "                # Form the complete URL with the selected date\n",
    "                complete_url = f\"https://www.atptour.com/en/rankings/doubles?rankRange=1-5000&rankDate={date}\"\n",
    "\n",
    "                # Open the URL and wait for the content to load\n",
    "                driver2.get(complete_url)\n",
    "                time.sleep(5)  # Wait for 5 seconds for dynamic content to load, adjust as needed\n",
    "\n",
    "                # Get the page source and create BeautifulSoup object\n",
    "                page_source = driver2.page_source\n",
    "                soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "                # Continue with extracting data as before\n",
    "                rankings_table = soup.find(\"table\", {\"class\": \"mega-table\"})\n",
    "#                 rows = rankings_table.find_all(\"tr\")[1:]  # Skip the header row\n",
    "                rows = rankings_table.find_all(\"tr\")[1:11]  # Process only the first 10 rows\n",
    "\n",
    "\n",
    "                for row in rows:\n",
    "                    rank = row.find(\"td\", {\"class\": \"rank-cell\"}).text.strip()\n",
    "                    player_name = row.find(\"td\", {\"class\": \"player-cell\"}).text.strip()\n",
    "                    age = row.find(\"td\", {\"class\": \"age-cell\"}).text.strip()\n",
    "                    points = row.find(\"td\", {\"class\": \"points-cell\"}).text.strip()\n",
    "                    tournaments_played = row.find(\"td\", {\"class\": \"tourn-cell\"}).text.strip()\n",
    "\n",
    "                    # Extract player profile link\n",
    "                    player_profile_link = row.find(\"a\", href=True)[\"href\"]\n",
    "\n",
    "                    # Open the player profile URL\n",
    "                    with webdriver.Chrome(service=service) as driver3:\n",
    "                        driver3.get(f\"https://www.atptour.com{player_profile_link}\")\n",
    "                        time.sleep(5)  # Wait for 5 seconds for dynamic content to load, adjust as needed\n",
    "\n",
    "                        # Get the player profile page source and create BeautifulSoup object\n",
    "                        player_page_source = driver3.page_source\n",
    "                        soup_player = BeautifulSoup(player_page_source, \"html.parser\")\n",
    "\n",
    "                        # Continue with extracting player information\n",
    "                        birthplace_div = soup_player.find(\"div\", class_=\"table-value\")\n",
    "                        birthplace = birthplace_div.get_text(strip=True)\n",
    "                        city_birthplace, _, country_birthplace = birthplace.partition(\",\")\n",
    "\n",
    "                        turned_pro_label_div = soup_player.find(\"div\", class_=\"table-big-label\", text=\"Turned Pro\")\n",
    "                        year_pro_div = turned_pro_label_div.find_next_sibling(\"div\", class_=\"table-big-value\")\n",
    "                        year_pro = year_pro_div.get_text(strip=True)\n",
    "\n",
    "                        weight_span = soup_player.select_one(\"span.table-weight-kg-wrapper\")\n",
    "                        weight_kg = \"\"\n",
    "                        if weight_span:\n",
    "                            weight_kg = weight_span.get_text(strip=True).strip(\"()\")\n",
    "\n",
    "                        height_span = soup_player.select_one(\"span.table-height-cm-wrapper\")\n",
    "                        height_cm = \"\"\n",
    "                        if height_span:\n",
    "                            height_cm = height_span.get_text(strip=True).strip(\"()\")\n",
    "\n",
    "                        # Find the div with class \"table-value\" for hand and backhand\n",
    "                        hand_backhand_div = soup_player.find_all(\"div\", class_=\"table-value\")[1]\n",
    "\n",
    "                        # Extract hand and backhand information if available\n",
    "                        hand, backhand = \"\", \"\"\n",
    "                        if hand_backhand_div:\n",
    "                            hand, backhand = [item.strip() for item in hand_backhand_div.get_text(strip=True).split(\",\")]\n",
    "\n",
    "                        # Find the div with class \"table-value\" for coach information\n",
    "                        coach_div = soup_player.find_all(\"div\", class_=\"table-value\")[2]\n",
    "\n",
    "                        # Extract coach information if available\n",
    "                        coaches = coach_div.get_text(strip=True).split(\", \")\n",
    "                        coach1 = coaches[0]\n",
    "                        coach2 = \"\" if len(coaches) < 2 else coaches[1]\n",
    "\n",
    "\n",
    "                        writer.writerow([rank, player_name, age, points, tournaments_played, year_pro, weight_kg, height_cm, city_birthplace, country_birthplace, hand, backhand, coach1, coach2])\n",
    "\n",
    "        print(f\"Rank Range: 1-5000, Rank Date: {date} - Data saved to {filename}\")\n",
    "        print()\n",
    "\n",
    "finally:\n",
    "    # Don't forget to stop the Service once you are done.\n",
    "    service.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
