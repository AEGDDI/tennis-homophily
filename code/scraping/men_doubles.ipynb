{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3f982978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# LIBRARIES\n",
    "# =========================\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from getpass import getuser\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Selenium essentials\n",
    "from selenium.webdriver import Firefox\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "ac7529e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "b0c97e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dbg(*args):\n",
    "    if DEBUG:\n",
    "        print(*args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "84b1137e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "USER = getuser()\n",
    "\n",
    "OUTPUT_DIR = f\"C:/Users/{USER}/Documents/GitHub/tennis-homophily/data/atp\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "N_ROWS_DEFAULT = 1000\n",
    "\n",
    "PAGELOAD_TIMEOUT = 20\n",
    "IMPLICIT_WAIT = 2\n",
    "SLEEP_MIN, SLEEP_MAX = 0.6, 2.0\n",
    "RETRIES = 3\n",
    "\n",
    "DATEWEEKS_TOURNAMENTS: List[Tuple[str, str, str]] = [\n",
    "    (\"2018-01-15\", \"Australian Open\", \"2018\"), (\"2019-01-14\", \"Australian Open\", \"2019\"),\n",
    "    (\"2020-01-20\", \"Australian Open\", \"2020\"), (\"2021-02-08\", \"Australian Open\", \"2021\"),\n",
    "    (\"2022-01-17\", \"Australian Open\", \"2022\"), (\"2023-01-16\", \"Australian Open\", \"2023\"),\n",
    "    (\"2018-05-21\", \"Roland Garros\", \"2018\"), (\"2019-05-20\", \"Roland Garros\", \"2019\"),\n",
    "    (\"2020-09-21\", \"Roland Garros\", \"2020\"), (\"2021-05-24\", \"Roland Garros\", \"2021\"),\n",
    "    (\"2022-05-16\", \"Roland Garros\", \"2022\"), (\"2023-05-22\", \"Roland Garros\", \"2023\"),\n",
    "    (\"2018-09-24\", \"US Open\", \"2018\"), (\"2019-08-26\", \"US Open\", \"2019\"),\n",
    "    (\"2020-08-31\", \"US Open\", \"2020\"), (\"2021-08-30\", \"US Open\", \"2021\"),\n",
    "    (\"2022-08-22\", \"US Open\", \"2022\"), (\"2023-08-28\", \"US Open\", \"2023\"),\n",
    "    (\"2018-07-02\", \"Wimbledon\", \"2018\"), (\"2019-07-01\", \"Wimbledon\", \"2019\"),\n",
    "    (\"2021-06-28\", \"Wimbledon\", \"2021\"), (\"2022-06-27\", \"Wimbledon\", \"2022\"),\n",
    "    (\"2023-07-03\", \"Wimbledon\", \"2023\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ee08c68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# DRIVER + UTILS\n",
    "# =========================\n",
    "def configure_driver(headless: bool = True) -> Firefox:\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "\n",
    "    # Your local geckodriver path\n",
    "    service = Service(executable_path=f\"C:/Users/{USER}/Downloads/geckodriver.exe\")\n",
    "\n",
    "    drv = Firefox(service=service, options=opts)\n",
    "    drv.set_page_load_timeout(PAGELOAD_TIMEOUT)\n",
    "    drv.implicitly_wait(IMPLICIT_WAIT)\n",
    "    return drv\n",
    "\n",
    "\n",
    "def random_sleep(min_seconds: float = SLEEP_MIN, max_seconds: float = SLEEP_MAX) -> None:\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "90ade2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# RANKINGS TABLE PARSER\n",
    "# =========================\n",
    "def parse_rankings_table(html: str, max_rows: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse rankings HTML to extract: Rank, Player, Player Profile Link.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tbody = soup.find(\"tbody\")\n",
    "    if not tbody:\n",
    "        return []\n",
    "\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "    out: List[Dict] = []\n",
    "\n",
    "    for row in rows[:max_rows]:\n",
    "        rank_cell = row.find(\"td\", class_=\"rank\")\n",
    "        player_cell = row.find(\"td\", class_=\"player\")\n",
    "\n",
    "        if not rank_cell:\n",
    "            rank_cell = row.find(\"td\", class_=\"rank bold heavy tiny-cell\")\n",
    "        if not player_cell:\n",
    "            player_cell = row.find(\"td\", class_=\"player bold heavy large-cell\")\n",
    "\n",
    "        if not (rank_cell and player_cell):\n",
    "            continue\n",
    "\n",
    "        link_tag = player_cell.find(\"a\")\n",
    "        profile_link = \"\"\n",
    "        if link_tag and link_tag.get(\"href\"):\n",
    "            href = link_tag.get(\"href\").strip()\n",
    "            profile_link = href if href.startswith(\"http\") else \"https://www.atptour.com\" + href\n",
    "\n",
    "        out.append({\n",
    "            \"Rank\": rank_cell.get_text(strip=True),\n",
    "            \"Player\": player_cell.get_text(strip=True),\n",
    "            \"Player Profile Link\": profile_link\n",
    "        })\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "cf93e963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# PROFILE PARSERS (DOUBLES)\n",
    "# =========================\n",
    "def parse_player_profile_overview(soup: BeautifulSoup) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract tab-dependent overview: W-L YTD/Career and Titles YTD/Career.\n",
    "    WARNING: values depend on the currently active tab (Singles/Doubles).\n",
    "    \"\"\"\n",
    "    data: Dict = {}\n",
    "    wins = soup.find_all(\"div\", class_=\"wins\")\n",
    "    titles = soup.find_all(\"div\", class_=\"titles\")\n",
    "\n",
    "    for timerange, win in zip([\"YTD\", \"Career\"], wins):\n",
    "        data[f\"W-L {timerange}\"] = win.get_text(strip=True).replace(\"W-L\", \"\").strip()\n",
    "\n",
    "    for timerange, title in zip([\"YTD\", \"Career\"], titles):\n",
    "        data[f\"Titles {timerange}\"] = title.get_text(strip=True).replace(\"Titles\", \"\").strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_player_profile_details(soup: BeautifulSoup) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract key/value pairs from ul.pd_left and ul.pd_right (DOB, Turned Pro, Height, Weight, Plays, Coach, etc.)\n",
    "    \"\"\"\n",
    "    data: Dict = {}\n",
    "    for html_class in (\"pd_left\", \"pd_right\"):\n",
    "        section = soup.find(\"ul\", class_=html_class)\n",
    "        if not section:\n",
    "            continue\n",
    "        for item in section.find_all(\"li\"):\n",
    "            spans = item.find_all(\"span\")\n",
    "            if len(spans) > 1:\n",
    "                key = spans[0].get_text(strip=True)\n",
    "                if key == \"Follow player\":\n",
    "                    continue\n",
    "                value = spans[1].get_text(strip=True)\n",
    "                data[key] = value\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1632887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ROBUST TAB SWITCHING\n",
    "# =========================\n",
    "def click_tab(driver: Firefox, tab_name: str, timeout: int = 12) -> None:\n",
    "    \"\"\"\n",
    "    Click Singles/Doubles tab and WAIT until it becomes active.\n",
    "    \"\"\"\n",
    "    wait = WebDriverWait(driver, timeout)\n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a.tab-switcher-link\")))\n",
    "\n",
    "    tab_xpath = f\"//a[contains(@class,'tab-switcher-link')][contains(normalize-space(.), '{tab_name}')]\"\n",
    "    el = wait.until(EC.element_to_be_clickable((By.XPATH, tab_xpath)))\n",
    "    driver.execute_script(\"arguments[0].click();\", el)\n",
    "\n",
    "    # Wait until active/selected (ATP markup varies; check common patterns)\n",
    "    def _is_active(drv):\n",
    "        try:\n",
    "            e = drv.find_element(By.XPATH, tab_xpath)\n",
    "            cls = (e.get_attribute(\"class\") or \"\").lower()\n",
    "            aria = (e.get_attribute(\"aria-selected\") or \"\").lower()\n",
    "            return (\"active\" in cls) or (aria == \"true\")\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    wait.until(_is_active)\n",
    "    time.sleep(0.5)  # buffer for re-render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "329ebbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# DEBUGGING + WAITING UTILS\n",
    "# =========================\n",
    "\n",
    "\n",
    "def dump_dom_snapshot(driver, label: str, enabled: bool = True, max_chars: int = 800):\n",
    "    \"\"\"Print small, safe snippets that help confirm we are on the right tab.\"\"\"\n",
    "    if not enabled:\n",
    "        return\n",
    "    html = driver.page_source\n",
    "    dbg(f\"\\n--- SNAPSHOT [{label}] ---\", enabled)\n",
    "    dbg(f\"URL: {driver.current_url}\", enabled)\n",
    "    dbg(f\"HTML length: {len(html)}\", enabled)\n",
    "    # quick “fingerprints”\n",
    "    m = re.search(r\"Career High Rank\", html)\n",
    "    dbg(f\"Contains 'Career High Rank'? {bool(m)}\", enabled)\n",
    "    dbg(f\"First {max_chars} chars:\\n{html[:max_chars]}\\n\", enabled)\n",
    "\n",
    "def active_tab_name(driver) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Tries to infer which tab is active.\n",
    "    This is heuristic because ATP-like pages vary.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tabs = driver.find_elements(By.CSS_SELECTOR, \"a.tab-switcher-link\")\n",
    "        for t in tabs:\n",
    "            cls = (t.get_attribute(\"class\") or \"\").lower()\n",
    "            if \"active\" in cls or \"is-active\" in cls or \"selected\" in cls:\n",
    "                return t.text.strip()\n",
    "        # fallback: return all tab texts if no active class\n",
    "        return None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def wait_for_tab_content_change(driver, before_html: str, timeout: float = 8.0) -> bool:\n",
    "    \"\"\"Wait until page_source changes meaningfully after a tab click.\"\"\"\n",
    "    end = time.time() + timeout\n",
    "    while time.time() < end:\n",
    "        after_html = driver.page_source\n",
    "        if after_html != before_html and abs(len(after_html) - len(before_html)) > 50:\n",
    "            return True\n",
    "        time.sleep(0.15)\n",
    "    return False\n",
    "\n",
    "def print_context_around(html: str, needle: str, window: int = 400, enabled: bool = True):\n",
    "    if not enabled:\n",
    "        return\n",
    "    idx = html.find(needle)\n",
    "    dbg(f\"[CTX] '{needle}' index in HTML: {idx}\")\n",
    "    if idx == -1:\n",
    "        return\n",
    "    start = max(0, idx - window)\n",
    "    end = min(len(html), idx + len(needle) + window)\n",
    "    snippet = html[start:end]\n",
    "    dbg(f\"[CTX] HTML around '{needle}' (len={len(snippet)}):\\n{snippet}\\n\")\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def print_text_context(html: str, needle: str, window: int = 200, enabled: bool = True):\n",
    "    if not enabled:\n",
    "        return\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    text = soup.get_text(\" \", strip=True)\n",
    "    idx = text.find(needle)\n",
    "    dbg(f\"[CTX-TXT] '{needle}' index in TEXT: {idx}\")\n",
    "    if idx == -1:\n",
    "        return\n",
    "    start = max(0, idx - window)\n",
    "    end = min(len(text), idx + len(needle) + window)\n",
    "    dbg(f\"[CTX-TXT] TEXT around '{needle}':\\n{text[start:end]}\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1959f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# SINGLES: DOM LABEL->VALUE EXTRACTOR + PARSER\n",
    "# =========================\n",
    "def extract_label_value_stats(soup: BeautifulSoup) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Generic extractor of label/value stat blocks.\n",
    "    We try several selector patterns used across ATP layouts.\n",
    "    \"\"\"\n",
    "    stats: Dict[str, str] = {}\n",
    "\n",
    "    # Candidate containers\n",
    "    candidates = []\n",
    "    candidates += soup.select(\"div.stat\")\n",
    "    candidates += soup.select(\"div.stat-item\")\n",
    "    candidates += soup.select(\"div.player-stats-item\")\n",
    "    if not candidates:\n",
    "        candidates = soup.select(\"div[class*='stat']\")\n",
    "\n",
    "    for c in candidates:\n",
    "        label_el = (\n",
    "            c.select_one(\".stat-label\")\n",
    "            or c.select_one(\".label\")\n",
    "            or c.select_one(\"span.label\")\n",
    "            or c.find(\"div\", class_=re.compile(\"label\", re.I))\n",
    "            or c.find(\"span\", class_=re.compile(\"label\", re.I))\n",
    "        )\n",
    "        value_el = (\n",
    "            c.select_one(\".stat-value\")\n",
    "            or c.select_one(\".value\")\n",
    "            or c.select_one(\"span.value\")\n",
    "            or c.find(\"div\", class_=re.compile(\"value\", re.I))\n",
    "            or c.find(\"span\", class_=re.compile(\"value\", re.I))\n",
    "        )\n",
    "\n",
    "        if not label_el or not value_el:\n",
    "            continue\n",
    "\n",
    "        label = label_el.get_text(\" \", strip=True)\n",
    "        value = value_el.get_text(\" \", strip=True)\n",
    "\n",
    "        if label and value:\n",
    "            stats[label] = value\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "dc552be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_singles_from_page_source(html: str) -> dict:\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    out = {\n",
    "        \"Singles_Career_High_Rank\": None,\n",
    "        \"Singles_Date_Career_High_Rank\": None,\n",
    "        \"Singles_WL_Career\": None,\n",
    "        \"Singles_Titles_Career\": None,\n",
    "    }\n",
    "\n",
    "    # 1) pick the correct \"Career\" box among multiple player-stats-details\n",
    "    boxes = soup.select(\"div.player-stats-details\")\n",
    "    career_box = None\n",
    "    for b in boxes:\n",
    "        t = b.select_one(\"div.type\")\n",
    "        if t and t.get_text(\" \", strip=True).lower() == \"career\":\n",
    "            career_box = b\n",
    "            break\n",
    "\n",
    "    if career_box is None:\n",
    "        # fallback: sometimes the word Career is present in the box text\n",
    "        for b in boxes:\n",
    "            if \"Career High Rank\" in b.get_text(\" \", strip=True):\n",
    "                career_box = b\n",
    "                break\n",
    "\n",
    "    if career_box is None:\n",
    "        return out\n",
    "\n",
    "    # 2) Career High Rank (value comes before label span)\n",
    "    stat = career_box.select_one(\"div.stat\")\n",
    "    if stat:\n",
    "        full = stat.get_text(\" \", strip=True)  # e.g. \"41 Career High Rank (2010.04.12)\"\n",
    "        m_rank = re.search(r\"\\b(\\d{1,4})\\b\", full)\n",
    "        if m_rank:\n",
    "            out[\"Singles_Career_High_Rank\"] = int(m_rank.group(1))\n",
    "\n",
    "        m_date = re.search(r\"\\((\\d{4}\\.\\d{2}\\.\\d{2})\\)\", full)\n",
    "        if m_date:\n",
    "            out[\"Singles_Date_Career_High_Rank\"] = m_date.group(1)\n",
    "\n",
    "    # 3) Career W-L\n",
    "    wins = career_box.select_one(\"div.wins\")\n",
    "    if wins:\n",
    "        full = wins.get_text(\" \", strip=True)  # e.g. \"97 - 130 W-L\"\n",
    "        m = re.search(r\"\\b(\\d+)\\s*-\\s*(\\d+)\\b\", full)\n",
    "        if m:\n",
    "            out[\"Singles_WL_Career\"] = f\"{m.group(1)}-{m.group(2)}\"\n",
    "\n",
    "    # 4) Career Titles\n",
    "    titles = career_box.select_one(\"div.titles\")\n",
    "    if titles:\n",
    "        full = titles.get_text(\" \", strip=True)  # e.g. \"0 Titles\"\n",
    "        m = re.search(r\"\\b(\\d+)\\b\", full)\n",
    "        if m:\n",
    "            out[\"Singles_Titles_Career\"] = int(m.group(1))\n",
    "\n",
    "    return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "367bfc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# SCRAPE RANKINGS PAGE (DOUBLES)\n",
    "# =========================\n",
    "def scrape_player_urls(dateweek: str, tournament: str, year: str, n_rows: int = N_ROWS_DEFAULT) -> List[Dict]:\n",
    "    ranking_url = f\"https://www.atptour.com/en/rankings/doubles?RankRange=1-1000&Region=all&DateWeek={dateweek}\"\n",
    "    driver = configure_driver()\n",
    "    try:\n",
    "        driver.get(ranking_url)\n",
    "        random_sleep()\n",
    "\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"tbody\")))\n",
    "        players = parse_rankings_table(driver.page_source, n_rows)\n",
    "\n",
    "        # \"Tourns\" dynamic column via Selenium\n",
    "        tourn_cells = driver.find_elements(By.XPATH, \"//td[contains(@class,'tourns')]\")\n",
    "        tourn_texts = [cell.text.strip() for cell in tourn_cells[:len(players)]]\n",
    "\n",
    "        merged = []\n",
    "        for i, p in enumerate(players):\n",
    "            pp = p.copy()\n",
    "            pp[\"Tourns\"] = tourn_texts[i] if i < len(tourn_texts) else \"\"\n",
    "            pp[\"Tournament\"] = tournament\n",
    "            pp[\"Year\"] = year\n",
    "            pp[\"DateWeek\"] = dateweek\n",
    "            merged.append(pp)\n",
    "\n",
    "        return merged\n",
    "\n",
    "    except (WebDriverException, TimeoutException) as e:\n",
    "        print(f\"[Rankings] Error for {dateweek} {tournament} {year}: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "5c8243a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# SCRAPE PLAYER PROFILE (DOUBLES + SINGLES)\n",
    "# =========================\n",
    "def scrape_player_profile(profile_link: str, debug: bool = True) -> Dict:\n",
    "    driver = configure_driver()\n",
    "    try:\n",
    "        for attempt in range(1, RETRIES + 1):\n",
    "            try:\n",
    "                dbg(f\"\\n[Profile] Attempt {attempt}/{RETRIES} -> {profile_link}\", debug)\n",
    "                driver.get(profile_link)\n",
    "\n",
    "                WebDriverWait(driver, 12).until(\n",
    "                    EC.presence_of_all_elements_located(\n",
    "                        (By.CSS_SELECTOR, \"ul.pd_left, ul.pd_right, a.tab-switcher-link\")\n",
    "                    )\n",
    "                )\n",
    "                dbg(\"[Profile] Base elements loaded (pd_left/pd_right/tabs).\", debug)\n",
    "\n",
    "                data: Dict = {}\n",
    "\n",
    "                # 1) DOUBLES first\n",
    "                try:\n",
    "                    before = driver.page_source\n",
    "                    dbg(\"[Doubles] Clicking tab...\", debug)\n",
    "                    click_tab(driver, \"Doubles\", timeout=12)\n",
    "                    changed = wait_for_tab_content_change(driver, before, timeout=8.0)\n",
    "                    dbg(f\"[Doubles] DOM changed after click? {changed}\", debug)\n",
    "                    dbg(f\"[Doubles] Active tab (heuristic): {active_tab_name(driver)}\", debug)\n",
    "                except Exception as e:\n",
    "                    dbg(f\"[Doubles] Click failed (continuing): {repr(e)}\", debug)\n",
    "\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                doubles_overview = parse_player_profile_overview(soup)\n",
    "                doubles_details = parse_player_profile_details(soup)\n",
    "\n",
    "                data.update({f\"Doubles_{k}\": v for k, v in doubles_overview.items()})\n",
    "                data.update(doubles_details)\n",
    "\n",
    "                dbg(f\"[Doubles] Parsed overview keys: {list(doubles_overview.keys())}\", debug)\n",
    "                dbg(f\"[Doubles] Parsed details keys: {list(doubles_details.keys())}\", debug)\n",
    "\n",
    "                # 2) SINGLES (debuggable)\n",
    "                singles_data = {\n",
    "                    \"Singles_Career_High_Rank\": None,\n",
    "                    \"Singles_WL_Career\": None,\n",
    "                    \"Singles_Titles_Career\": None\n",
    "                }\n",
    "\n",
    "                try:\n",
    "                    dbg(\"[Singles] Clicking tab...\", debug)\n",
    "                    before = driver.page_source\n",
    "                    click_tab(driver, \"Singles\", timeout=12)\n",
    "\n",
    "                    changed = wait_for_tab_content_change(driver, before, timeout=10.0)\n",
    "                    dbg(f\"[Singles] DOM changed after click? {changed}\", debug)\n",
    "                    dbg(f\"[Singles] Active tab (heuristic): {active_tab_name(driver)}\", debug)\n",
    "\n",
    "                    # IMPORTANT: wait for something *specific* to singles panel\n",
    "                    # If your site has a clear container, replace selector accordingly.\n",
    "                    # Example: \"div.player-stats-details\" (you mentioned this earlier)\n",
    "                    wait_selectors = [\n",
    "                        \"div.player-stats-details\",\n",
    "                        \"div.wins\", \"div.titles\",\n",
    "                        \"div[class*='stat']\"\n",
    "                    ]\n",
    "\n",
    "                    found_any = False\n",
    "                    for sel in wait_selectors:\n",
    "                        try:\n",
    "                            WebDriverWait(driver, 6).until(\n",
    "                                EC.presence_of_element_located((By.CSS_SELECTOR, sel))\n",
    "                            )\n",
    "                            dbg(f\"[Singles] Found selector: {sel}\", debug)\n",
    "                            found_any = True\n",
    "                            break\n",
    "                        except Exception:\n",
    "                            continue\n",
    "\n",
    "                    dbg(f\"[Singles] Found any expected selector? {found_any}\", debug)\n",
    "                    dump_dom_snapshot(driver, \"after_singles_click\", enabled=debug, max_chars=600)\n",
    "\n",
    "                    html = driver.page_source\n",
    "                    print_context_around(html, \"Career High Rank\", enabled=debug)\n",
    "                    print_text_context(html, needle=\"Career High Rank\", enabled=debug)\n",
    "                    singles_data = parse_singles_from_page_source(html)\n",
    "\n",
    "                    if debug:\n",
    "                        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                        boxes = soup.select(\"div.player-stats-details\")\n",
    "                        print(f\"[TEST] player-stats-details count: {len(boxes)}\")\n",
    "                        for i, b in enumerate(boxes):\n",
    "                            t = b.select_one(\"div.type\")\n",
    "                            dbg(f\"[TEST] box[{i}] type:\", t.get_text(\" \", strip=True) if t else None)\n",
    "                            dbg(f\"[TEST] box[{i}] text head:\", b.get_text(\" \", strip=True)[:80])\n",
    "\n",
    "\n",
    "                    dbg(f\"[Singles] Parsed singles_data: {singles_data}\", debug)\n",
    "\n",
    "                    # sanity checks\n",
    "                    if all(v is None for v in singles_data.values()):\n",
    "                        dbg(\"[Singles][WARN] All parsed Singles values are None. Likely wrong DOM/text.\", debug)\n",
    "\n",
    "                except Exception as e:\n",
    "                    dbg(f\"[Singles] Failed: {repr(e)}\", debug)\n",
    "                    # keep fallback None values\n",
    "\n",
    "                data.update(singles_data)\n",
    "                return data\n",
    "\n",
    "            except (TimeoutException, WebDriverException) as e:\n",
    "                dbg(f\"[Profile] Attempt {attempt}/{RETRIES} failed: {repr(e)}\", debug)\n",
    "                random_sleep(1.5, 3.0)\n",
    "\n",
    "        dbg(f\"[Profile] Failed after {RETRIES} attempts: {profile_link}\", debug)\n",
    "        return {}\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "9a0998b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Week] 2018-01-15 Australian Open 2018: found 1 players.\n",
      "[TEST] player-stats-details count: 2\n",
      "    - (1/1) L. Kubot processed.\n",
      "[Save] 1 rows saved to: C:/Users/ALESSANDRO/Documents/GitHub/tennis-homophily/data/atp\\ranking_doubles_2018-01-15_TEST.xlsx\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# SAVE TO EXCEL\n",
    "# =========================\n",
    "def save_player_info_to_excel(\n",
    "    records: List[Dict],\n",
    "    dateweek: str,\n",
    "    out_dir: str = OUTPUT_DIR,\n",
    "    suffix: str = \"\"\n",
    ") -> str:\n",
    "    if not records:\n",
    "        print(f\"[Save] No data to save for {dateweek}.\")\n",
    "        return \"\"\n",
    "\n",
    "    df_out = pd.DataFrame(records)\n",
    "    fname = f\"ranking_doubles_{dateweek}{suffix}.xlsx\"\n",
    "    out_path = os.path.join(out_dir, fname)\n",
    "\n",
    "    df_out.to_excel(out_path, index=False)\n",
    "    print(f\"[Save] {len(df_out)} rows saved to: {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# =========================\n",
    "# FULL RUN\n",
    "# =========================\n",
    "def run_full_schedule(\n",
    "    dateweeks_tournaments=DATEWEEKS_TOURNAMENTS,\n",
    "    n_rows=N_ROWS_DEFAULT,\n",
    "    test_run: bool = False\n",
    ") -> None:\n",
    "    for dateweek, tournament, year in dateweeks_tournaments:\n",
    "        try:\n",
    "            players = scrape_player_urls(dateweek, tournament, year, n_rows=n_rows)\n",
    "            print(f\"[Week] {dateweek} {tournament} {year}: found {len(players)} players.\")\n",
    "\n",
    "            for idx, p in enumerate(players, start=1):\n",
    "                link = p.get(\"Player Profile Link\") or \"\"\n",
    "                if link:\n",
    "                    profile = scrape_player_profile(link)\n",
    "                    p.update(profile)\n",
    "\n",
    "                print(f\"    - ({idx}/{len(players)}) {p.get('Player','?')} processed.\")\n",
    "                random_sleep(0.5, 1.3)\n",
    "\n",
    "            if players:\n",
    "                suffix = \"_TEST\" if test_run else \"\"\n",
    "                save_player_info_to_excel(players, dateweek, suffix=suffix)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Week] Unexpected error for {dateweek} {tournament} {year}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "# =========================\n",
    "# TEST RUN (10 players)\n",
    "# =========================\n",
    "test_subset = DATEWEEKS_TOURNAMENTS[:1]\n",
    "run_full_schedule(test_subset, n_rows=1, test_run=True)\n",
    "\n",
    "# Full run when ready:\n",
    "# run_full_schedule(DATEWEEKS_TOURNAMENTS, n_rows=1000, test_run=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
