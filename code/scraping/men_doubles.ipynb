{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a0998b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Week] 2018-01-15 Australian Open 2018: found 10 players.\n",
      "    - (1/10) L. Kubot processed.\n",
      "    - (2/10) M. Melo processed.\n",
      "    - (3/10) H. Kontinen processed.\n",
      "    - (4/10) J. Peers processed.\n",
      "    - (5/10) I. Dodig processed.\n",
      "    - (6/10) N. Mahut processed.\n",
      "    - (7/10) J. Rojer processed.\n",
      "    - (8/10) H. Tecau processed.\n",
      "    - (9/10) J. Murray processed.\n",
      "    - (10/10) B. Soares processed.\n",
      "[Save] 10 rows saved to: C:/Users/ALESSANDRO/Documents/GitHub/tennis-homophily/data/atp\\ranking_doubles_2018-01-15_TEST.xlsx\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# FULL CODE (REWRITTEN) â€” Doubles scraping preserved + Singles info fixed\n",
    "# - Robust tab switching (waits until active)\n",
    "# - Singles parsed via DOM label->value extraction + regex fallback\n",
    "# - Doubles overview is prefixed to avoid being overwritten\n",
    "# ============================================================\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from getpass import getuser\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Selenium essentials\n",
    "from selenium.webdriver import Firefox\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n",
    "\n",
    "\n",
    "# =========================\n",
    "# CONFIG\n",
    "# =========================\n",
    "USER = getuser()\n",
    "\n",
    "OUTPUT_DIR = f\"C:/Users/{USER}/Documents/GitHub/tennis-homophily/data/atp\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "N_ROWS_DEFAULT = 1000\n",
    "\n",
    "PAGELOAD_TIMEOUT = 20\n",
    "IMPLICIT_WAIT = 2\n",
    "SLEEP_MIN, SLEEP_MAX = 0.6, 2.0\n",
    "RETRIES = 3\n",
    "\n",
    "DATEWEEKS_TOURNAMENTS: List[Tuple[str, str, str]] = [\n",
    "    (\"2018-01-15\", \"Australian Open\", \"2018\"), (\"2019-01-14\", \"Australian Open\", \"2019\"),\n",
    "    (\"2020-01-20\", \"Australian Open\", \"2020\"), (\"2021-02-08\", \"Australian Open\", \"2021\"),\n",
    "    (\"2022-01-17\", \"Australian Open\", \"2022\"), (\"2023-01-16\", \"Australian Open\", \"2023\"),\n",
    "    (\"2018-05-21\", \"Roland Garros\", \"2018\"), (\"2019-05-20\", \"Roland Garros\", \"2019\"),\n",
    "    (\"2020-09-21\", \"Roland Garros\", \"2020\"), (\"2021-05-24\", \"Roland Garros\", \"2021\"),\n",
    "    (\"2022-05-16\", \"Roland Garros\", \"2022\"), (\"2023-05-22\", \"Roland Garros\", \"2023\"),\n",
    "    (\"2018-09-24\", \"US Open\", \"2018\"), (\"2019-08-26\", \"US Open\", \"2019\"),\n",
    "    (\"2020-08-31\", \"US Open\", \"2020\"), (\"2021-08-30\", \"US Open\", \"2021\"),\n",
    "    (\"2022-08-22\", \"US Open\", \"2022\"), (\"2023-08-28\", \"US Open\", \"2023\"),\n",
    "    (\"2018-07-02\", \"Wimbledon\", \"2018\"), (\"2019-07-01\", \"Wimbledon\", \"2019\"),\n",
    "    (\"2021-06-28\", \"Wimbledon\", \"2021\"), (\"2022-06-27\", \"Wimbledon\", \"2022\"),\n",
    "    (\"2023-07-03\", \"Wimbledon\", \"2023\")\n",
    "]\n",
    "\n",
    "\n",
    "# =========================\n",
    "# DRIVER + UTILS\n",
    "# =========================\n",
    "def configure_driver(headless: bool = True) -> Firefox:\n",
    "    opts = Options()\n",
    "    if headless:\n",
    "        opts.add_argument(\"--headless=new\")\n",
    "\n",
    "    # Your local geckodriver path\n",
    "    service = Service(executable_path=f\"C:/Users/{USER}/Downloads/geckodriver.exe\")\n",
    "\n",
    "    drv = Firefox(service=service, options=opts)\n",
    "    drv.set_page_load_timeout(PAGELOAD_TIMEOUT)\n",
    "    drv.implicitly_wait(IMPLICIT_WAIT)\n",
    "    return drv\n",
    "\n",
    "\n",
    "def random_sleep(min_seconds: float = SLEEP_MIN, max_seconds: float = SLEEP_MAX) -> None:\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RANKINGS TABLE PARSER\n",
    "# =========================\n",
    "def parse_rankings_table(html: str, max_rows: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse rankings HTML to extract: Rank, Player, Player Profile Link.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tbody = soup.find(\"tbody\")\n",
    "    if not tbody:\n",
    "        return []\n",
    "\n",
    "    rows = tbody.find_all(\"tr\")\n",
    "    out: List[Dict] = []\n",
    "\n",
    "    for row in rows[:max_rows]:\n",
    "        rank_cell = row.find(\"td\", class_=\"rank\")\n",
    "        player_cell = row.find(\"td\", class_=\"player\")\n",
    "\n",
    "        if not rank_cell:\n",
    "            rank_cell = row.find(\"td\", class_=\"rank bold heavy tiny-cell\")\n",
    "        if not player_cell:\n",
    "            player_cell = row.find(\"td\", class_=\"player bold heavy large-cell\")\n",
    "\n",
    "        if not (rank_cell and player_cell):\n",
    "            continue\n",
    "\n",
    "        link_tag = player_cell.find(\"a\")\n",
    "        profile_link = \"\"\n",
    "        if link_tag and link_tag.get(\"href\"):\n",
    "            href = link_tag.get(\"href\").strip()\n",
    "            profile_link = href if href.startswith(\"http\") else \"https://www.atptour.com\" + href\n",
    "\n",
    "        out.append({\n",
    "            \"Rank\": rank_cell.get_text(strip=True),\n",
    "            \"Player\": player_cell.get_text(strip=True),\n",
    "            \"Player Profile Link\": profile_link\n",
    "        })\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# PROFILE PARSERS (DOUBLES)\n",
    "# =========================\n",
    "def parse_player_profile_overview(soup: BeautifulSoup) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract tab-dependent overview: W-L YTD/Career and Titles YTD/Career.\n",
    "    WARNING: values depend on the currently active tab (Singles/Doubles).\n",
    "    \"\"\"\n",
    "    data: Dict = {}\n",
    "    wins = soup.find_all(\"div\", class_=\"wins\")\n",
    "    titles = soup.find_all(\"div\", class_=\"titles\")\n",
    "\n",
    "    for timerange, win in zip([\"YTD\", \"Career\"], wins):\n",
    "        data[f\"W-L {timerange}\"] = win.get_text(strip=True).replace(\"W-L\", \"\").strip()\n",
    "\n",
    "    for timerange, title in zip([\"YTD\", \"Career\"], titles):\n",
    "        data[f\"Titles {timerange}\"] = title.get_text(strip=True).replace(\"Titles\", \"\").strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_player_profile_details(soup: BeautifulSoup) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract key/value pairs from ul.pd_left and ul.pd_right (DOB, Turned Pro, Height, Weight, Plays, Coach, etc.)\n",
    "    \"\"\"\n",
    "    data: Dict = {}\n",
    "    for html_class in (\"pd_left\", \"pd_right\"):\n",
    "        section = soup.find(\"ul\", class_=html_class)\n",
    "        if not section:\n",
    "            continue\n",
    "        for item in section.find_all(\"li\"):\n",
    "            spans = item.find_all(\"span\")\n",
    "            if len(spans) > 1:\n",
    "                key = spans[0].get_text(strip=True)\n",
    "                if key == \"Follow player\":\n",
    "                    continue\n",
    "                value = spans[1].get_text(strip=True)\n",
    "                data[key] = value\n",
    "    return data\n",
    "\n",
    "\n",
    "# =========================\n",
    "# ROBUST TAB SWITCHING\n",
    "# =========================\n",
    "def click_tab(driver: Firefox, tab_name: str, timeout: int = 12) -> None:\n",
    "    \"\"\"\n",
    "    Click Singles/Doubles tab and WAIT until it becomes active.\n",
    "    \"\"\"\n",
    "    wait = WebDriverWait(driver, timeout)\n",
    "    wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, \"a.tab-switcher-link\")))\n",
    "\n",
    "    tab_xpath = f\"//a[contains(@class,'tab-switcher-link')][contains(normalize-space(.), '{tab_name}')]\"\n",
    "    el = wait.until(EC.element_to_be_clickable((By.XPATH, tab_xpath)))\n",
    "    driver.execute_script(\"arguments[0].click();\", el)\n",
    "\n",
    "    # Wait until active/selected (ATP markup varies; check common patterns)\n",
    "    def _is_active(drv):\n",
    "        try:\n",
    "            e = drv.find_element(By.XPATH, tab_xpath)\n",
    "            cls = (e.get_attribute(\"class\") or \"\").lower()\n",
    "            aria = (e.get_attribute(\"aria-selected\") or \"\").lower()\n",
    "            return (\"active\" in cls) or (aria == \"true\")\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "    wait.until(_is_active)\n",
    "    time.sleep(0.5)  # buffer for re-render\n",
    "\n",
    "\n",
    "# =========================\n",
    "# SINGLES: DOM LABEL->VALUE EXTRACTOR + PARSER\n",
    "# =========================\n",
    "def extract_label_value_stats(soup: BeautifulSoup) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Generic extractor of label/value stat blocks.\n",
    "    We try several selector patterns used across ATP layouts.\n",
    "    \"\"\"\n",
    "    stats: Dict[str, str] = {}\n",
    "\n",
    "    # Candidate containers\n",
    "    candidates = []\n",
    "    candidates += soup.select(\"div.stat\")\n",
    "    candidates += soup.select(\"div.stat-item\")\n",
    "    candidates += soup.select(\"div.player-stats-item\")\n",
    "    if not candidates:\n",
    "        candidates = soup.select(\"div[class*='stat']\")\n",
    "\n",
    "    for c in candidates:\n",
    "        label_el = (\n",
    "            c.select_one(\".stat-label\")\n",
    "            or c.select_one(\".label\")\n",
    "            or c.select_one(\"span.label\")\n",
    "            or c.find(\"div\", class_=re.compile(\"label\", re.I))\n",
    "            or c.find(\"span\", class_=re.compile(\"label\", re.I))\n",
    "        )\n",
    "        value_el = (\n",
    "            c.select_one(\".stat-value\")\n",
    "            or c.select_one(\".value\")\n",
    "            or c.select_one(\"span.value\")\n",
    "            or c.find(\"div\", class_=re.compile(\"value\", re.I))\n",
    "            or c.find(\"span\", class_=re.compile(\"value\", re.I))\n",
    "        )\n",
    "\n",
    "        if not label_el or not value_el:\n",
    "            continue\n",
    "\n",
    "        label = label_el.get_text(\" \", strip=True)\n",
    "        value = value_el.get_text(\" \", strip=True)\n",
    "\n",
    "        if label and value:\n",
    "            stats[label] = value\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def parse_singles_from_page_source(html: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Parse Singles stats after Singles tab is active.\n",
    "    1) DOM label/value extraction\n",
    "    2) fallback: normalized text regex\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    out = {\n",
    "        \"Singles_Career_High_Rank\": None,\n",
    "        \"Singles_WL_Career\": None,\n",
    "        \"Singles_Titles_Career\": None,\n",
    "    }\n",
    "\n",
    "    # ---- 1) DOM extraction ----\n",
    "    stats = extract_label_value_stats(soup)\n",
    "\n",
    "    label_map = {\n",
    "        \"Career High Rank\": \"Singles_Career_High_Rank\",\n",
    "        \"Career High Ranking\": \"Singles_Career_High_Rank\",\n",
    "        \"Career W-L\": \"Singles_WL_Career\",\n",
    "        \"Career W/L\": \"Singles_WL_Career\",\n",
    "        \"Career Win/Loss\": \"Singles_WL_Career\",\n",
    "        \"Career Titles\": \"Singles_Titles_Career\",\n",
    "    }\n",
    "\n",
    "    for k, v in stats.items():\n",
    "        k_norm = k.strip()\n",
    "        if k_norm in label_map:\n",
    "            out[label_map[k_norm]] = v\n",
    "\n",
    "    # Clean numeric formats\n",
    "    if out[\"Singles_Career_High_Rank\"] is not None:\n",
    "        m = re.search(r\"(\\d+)\", str(out[\"Singles_Career_High_Rank\"]))\n",
    "        out[\"Singles_Career_High_Rank\"] = int(m.group(1)) if m else None\n",
    "\n",
    "    if out[\"Singles_Titles_Career\"] is not None:\n",
    "        m = re.search(r\"(\\d+)\", str(out[\"Singles_Titles_Career\"]))\n",
    "        out[\"Singles_Titles_Career\"] = int(m.group(1)) if m else None\n",
    "\n",
    "    # If we got something, return\n",
    "    if any(v is not None for v in out.values()):\n",
    "        return out\n",
    "\n",
    "    # ---- 2) fallback regex on normalized text ----\n",
    "    text = soup.get_text(\" \", strip=True).replace(\"\\xa0\", \" \")\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    m = re.search(r\"Career High Rank[: ]+(\\d+)\", text, flags=re.IGNORECASE)\n",
    "    out[\"Singles_Career_High_Rank\"] = int(m.group(1)) if m else None\n",
    "\n",
    "    m = re.search(r\"Career\\s+W-?L[: ]+(\\d+)\\s*-\\s*(\\d+)\", text, flags=re.IGNORECASE)\n",
    "    out[\"Singles_WL_Career\"] = f\"{m.group(1)}-{m.group(2)}\" if m else None\n",
    "\n",
    "    m = re.search(r\"Career\\s+Titles[: ]+(\\d+)\", text, flags=re.IGNORECASE)\n",
    "    out[\"Singles_Titles_Career\"] = int(m.group(1)) if m else None\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "# =========================\n",
    "# SCRAPE RANKINGS PAGE (DOUBLES)\n",
    "# =========================\n",
    "def scrape_player_urls(dateweek: str, tournament: str, year: str, n_rows: int = N_ROWS_DEFAULT) -> List[Dict]:\n",
    "    ranking_url = f\"https://www.atptour.com/en/rankings/doubles?RankRange=1-1000&Region=all&DateWeek={dateweek}\"\n",
    "    driver = configure_driver()\n",
    "    try:\n",
    "        driver.get(ranking_url)\n",
    "        random_sleep()\n",
    "\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"tbody\")))\n",
    "        players = parse_rankings_table(driver.page_source, n_rows)\n",
    "\n",
    "        # \"Tourns\" dynamic column via Selenium\n",
    "        tourn_cells = driver.find_elements(By.XPATH, \"//td[contains(@class,'tourns')]\")\n",
    "        tourn_texts = [cell.text.strip() for cell in tourn_cells[:len(players)]]\n",
    "\n",
    "        merged = []\n",
    "        for i, p in enumerate(players):\n",
    "            pp = p.copy()\n",
    "            pp[\"Tourns\"] = tourn_texts[i] if i < len(tourn_texts) else \"\"\n",
    "            pp[\"Tournament\"] = tournament\n",
    "            pp[\"Year\"] = year\n",
    "            pp[\"DateWeek\"] = dateweek\n",
    "            merged.append(pp)\n",
    "\n",
    "        return merged\n",
    "\n",
    "    except (WebDriverException, TimeoutException) as e:\n",
    "        print(f\"[Rankings] Error for {dateweek} {tournament} {year}: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# SCRAPE PLAYER PROFILE (DOUBLES + SINGLES)\n",
    "# =========================\n",
    "def scrape_player_profile(profile_link: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Preserves doubles data and adds singles data.\n",
    "    - Doubles overview fields are prefixed as Doubles_*\n",
    "    - Singles fields are Singles_*\n",
    "    - Profile details (DOB, Plays, Coach, etc.) are kept unchanged\n",
    "    \"\"\"\n",
    "    driver = configure_driver()\n",
    "    try:\n",
    "        for attempt in range(1, RETRIES + 1):\n",
    "            try:\n",
    "                driver.get(profile_link)\n",
    "\n",
    "                WebDriverWait(driver, 12).until(\n",
    "                    EC.presence_of_all_elements_located(\n",
    "                        (By.CSS_SELECTOR, \"ul.pd_left, ul.pd_right, a.tab-switcher-link\")\n",
    "                    )\n",
    "                )\n",
    "                random_sleep(0.4, 1.0)\n",
    "\n",
    "                data: Dict = {}\n",
    "\n",
    "                # 1) DOUBLES first\n",
    "                try:\n",
    "                    click_tab(driver, \"Doubles\", timeout=12)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "                doubles_overview = parse_player_profile_overview(soup)\n",
    "                doubles_details = parse_player_profile_details(soup)\n",
    "\n",
    "                data.update({f\"Doubles_{k}\": v for k, v in doubles_overview.items()})\n",
    "                data.update(doubles_details)\n",
    "\n",
    "                # 2) SINGLES\n",
    "                try:\n",
    "                    click_tab(driver, \"Singles\", timeout=12)\n",
    "                    WebDriverWait(driver, 12).until(\n",
    "                        EC.presence_of_element_located((By.CSS_SELECTOR, \"div.wins, div.titles, div[class*='stat']\"))\n",
    "                    )\n",
    "                    singles_data = parse_singles_from_page_source(driver.page_source)\n",
    "                except Exception:\n",
    "                    singles_data = {\n",
    "                        \"Singles_Career_High_Rank\": None,\n",
    "                        \"Singles_WL_Career\": None,\n",
    "                        \"Singles_Titles_Career\": None\n",
    "                    }\n",
    "\n",
    "                data.update(singles_data)\n",
    "                return data\n",
    "\n",
    "            except (TimeoutException, WebDriverException) as e:\n",
    "                print(f\"[Profile] Attempt {attempt}/{RETRIES} failed: {e}\")\n",
    "                random_sleep(1.5, 3.0)\n",
    "\n",
    "        print(f\"[Profile] Failed after {RETRIES} attempts: {profile_link}\")\n",
    "        return {}\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "\n",
    "# =========================\n",
    "# SAVE TO EXCEL\n",
    "# =========================\n",
    "def save_player_info_to_excel(\n",
    "    records: List[Dict],\n",
    "    dateweek: str,\n",
    "    out_dir: str = OUTPUT_DIR,\n",
    "    suffix: str = \"\"\n",
    ") -> str:\n",
    "    if not records:\n",
    "        print(f\"[Save] No data to save for {dateweek}.\")\n",
    "        return \"\"\n",
    "\n",
    "    df_out = pd.DataFrame(records)\n",
    "    fname = f\"ranking_doubles_{dateweek}{suffix}.xlsx\"\n",
    "    out_path = os.path.join(out_dir, fname)\n",
    "\n",
    "    df_out.to_excel(out_path, index=False)\n",
    "    print(f\"[Save] {len(df_out)} rows saved to: {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# =========================\n",
    "# FULL RUN\n",
    "# =========================\n",
    "def run_full_schedule(\n",
    "    dateweeks_tournaments=DATEWEEKS_TOURNAMENTS,\n",
    "    n_rows=N_ROWS_DEFAULT,\n",
    "    test_run: bool = False\n",
    ") -> None:\n",
    "    for dateweek, tournament, year in dateweeks_tournaments:\n",
    "        try:\n",
    "            players = scrape_player_urls(dateweek, tournament, year, n_rows=n_rows)\n",
    "            print(f\"[Week] {dateweek} {tournament} {year}: found {len(players)} players.\")\n",
    "\n",
    "            for idx, p in enumerate(players, start=1):\n",
    "                link = p.get(\"Player Profile Link\") or \"\"\n",
    "                if link:\n",
    "                    profile = scrape_player_profile(link)\n",
    "                    p.update(profile)\n",
    "\n",
    "                print(f\"    - ({idx}/{len(players)}) {p.get('Player','?')} processed.\")\n",
    "                random_sleep(0.5, 1.3)\n",
    "\n",
    "            if players:\n",
    "                suffix = \"_TEST\" if test_run else \"\"\n",
    "                save_player_info_to_excel(players, dateweek, suffix=suffix)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Week] Unexpected error for {dateweek} {tournament} {year}: {e}\")\n",
    "            continue\n",
    "\n",
    "\n",
    "# =========================\n",
    "# TEST RUN (10 players)\n",
    "# =========================\n",
    "test_subset = DATEWEEKS_TOURNAMENTS[:1]\n",
    "run_full_schedule(test_subset, n_rows=10, test_run=True)\n",
    "\n",
    "# Full run when ready:\n",
    "# run_full_schedule(DATEWEEKS_TOURNAMENTS, n_rows=1000, test_run=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
