{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b15be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from getpass import getuser\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Selenium essentials\n",
    "from selenium.webdriver import Firefox\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.firefox.service import Service\n",
    "from selenium.webdriver.common.by import By          \n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, WebDriverException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b279fc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# User-based Windows paths\n",
    "USER = getuser()\n",
    "\n",
    "OUTPUT_DIR = f\"C:/Users/{USER}/Documents/GitHub/tennis-homophily/data/atp\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# How many players to scrape from the rankings page\n",
    "N_ROWS_DEFAULT = 5  # adjust as needed\n",
    "\n",
    "# Global timeouts and retries\n",
    "PAGELOAD_TIMEOUT = 20\n",
    "IMPLICIT_WAIT = 2\n",
    "SLEEP_MIN, SLEEP_MAX = 0.6, 2.0  # polite randomized delay\n",
    "RETRIES = 3  # number of retries for loading a page\n",
    "\n",
    "# The schedule of weeks/tournaments/years to scrape\n",
    "DATEWEEKS_TOURNAMENTS: List[Tuple[str, str, str]] = [\n",
    "    (\"2018-01-15\", \"Australian Open\", \"2018\"), (\"2019-01-14\", \"Australian Open\", \"2019\"),\n",
    "    (\"2020-01-20\", \"Australian Open\", \"2020\"), (\"2021-02-08\", \"Australian Open\", \"2021\"),\n",
    "    (\"2022-01-17\", \"Australian Open\", \"2022\"), (\"2023-01-16\", \"Australian Open\", \"2023\"),\n",
    "    (\"2018-05-21\", \"Roland Garros\", \"2018\"), (\"2019-05-20\", \"Roland Garros\", \"2019\"),\n",
    "    (\"2020-09-21\", \"Roland Garros\", \"2020\"), (\"2021-05-24\", \"Roland Garros\", \"2021\"),\n",
    "    (\"2022-05-16\", \"Roland Garros\", \"2022\"), (\"2023-05-22\", \"Roland Garros\", \"2023\"),\n",
    "    (\"2018-09-24\", \"US Open\", \"2018\"), (\"2019-08-26\", \"US Open\", \"2019\"),\n",
    "    (\"2020-08-31\", \"US Open\", \"2020\"), (\"2021-08-30\", \"US Open\", \"2021\"),\n",
    "    (\"2022-08-22\", \"US Open\", \"2022\"), (\"2023-08-28\", \"US Open\", \"2023\"),\n",
    "    (\"2018-07-02\", \"Wimbledon\", \"2018\"), (\"2019-07-01\", \"Wimbledon\", \"2019\"),\n",
    "    (\"2021-06-28\", \"Wimbledon\", \"2021\"), (\"2022-06-27\", \"Wimbledon\", \"2022\"),\n",
    "    (\"2023-07-03\", \"Wimbledon\", \"2023\")\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6ed130fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 2: Selenium driver configuration ---\n",
    "\n",
    "def configure_driver(headless: bool = True) -> Firefox:\n",
    "    opts = Options()\n",
    "    opts.add_argument(\"--headless=new\")  # headless; remove if you want to see the browser\n",
    "\n",
    "    # IMPORTANT: don't pass executable_path; Selenium Manager will fetch a correct geckodriver\n",
    "    service = Service()  # no path\n",
    "\n",
    "    drv = Firefox(service=service, options=opts)\n",
    "    drv.set_page_load_timeout(PAGELOAD_TIMEOUT)\n",
    "    drv.implicitly_wait(IMPLICIT_WAIT)\n",
    "    return drv\n",
    "\n",
    "\n",
    "def random_sleep(min_seconds: float = SLEEP_MIN, max_seconds: float = SLEEP_MAX) -> None:\n",
    "    time.sleep(random.uniform(min_seconds, max_seconds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f78e0f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 3: HTML parsing helpers ---\n",
    "\n",
    "def parse_rankings_table(html: str, max_rows: int) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Parse the rankings HTML (page_source) to extract:\n",
    "    Rank, Player, Player Profile Link. 'Tourns' will be filled using Selenium column later.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.find(\"tbody\")\n",
    "\n",
    "    if not table:\n",
    "        return []\n",
    "\n",
    "    rows = table.find_all(\"tr\")\n",
    "    out = []\n",
    "    for row in rows[:max_rows]:\n",
    "        rank_cell = row.find(\"td\", class_=\"rank\")\n",
    "        player_cell = row.find(\"td\", class_=\"player\")\n",
    "        # Fallback to more specific classes if needed:\n",
    "        if not rank_cell:\n",
    "            rank_cell = row.find(\"td\", class_=\"rank bold heavy tiny-cell\")\n",
    "        if not player_cell:\n",
    "            player_cell = row.find(\"td\", class_=\"player bold heavy large-cell\")\n",
    "\n",
    "        if not (rank_cell and player_cell):\n",
    "            continue\n",
    "\n",
    "        link_tag = player_cell.find(\"a\")\n",
    "        profile_link = \"\"\n",
    "        if link_tag and link_tag.get(\"href\"):\n",
    "            href = link_tag.get(\"href\").strip()\n",
    "            profile_link = href if href.startswith(\"http\") else \"https://www.atptour.com\" + href\n",
    "\n",
    "        out.append({\n",
    "            \"Rank\": rank_cell.get_text(strip=True),\n",
    "            \"Player\": player_cell.get_text(strip=True),\n",
    "            \"Player Profile Link\": profile_link\n",
    "        })\n",
    "    return out\n",
    "\n",
    "\n",
    "def parse_player_profile_overview(soup: BeautifulSoup) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract 'W-L YTD', 'W-L Career', 'Titles YTD', 'Titles Career'\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    wins = soup.find_all(\"div\", class_='wins')\n",
    "    titles = soup.find_all(\"div\", class_='titles')\n",
    "\n",
    "    for timerange, win in zip(['YTD', 'Career'], wins):\n",
    "        # original code stripped 4 chars; safer: split by whitespace or use replace\n",
    "        data[f'W-L {timerange}'] = win.get_text(strip=True).replace(\"W-L\", \"\").strip()\n",
    "\n",
    "    for timerange, title in zip(['YTD', 'Career'], titles):\n",
    "        data[f'Titles {timerange}'] = title.get_text(strip=True).replace(\"Titles\", \"\").strip()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_player_profile_details(soup: BeautifulSoup) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract key/value pairs from the left/right detail lists (ul.pd_left / ul.pd_right).\n",
    "    Skips 'Follow player'.\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    for html_class in (\"pd_left\", \"pd_right\"):\n",
    "        section = soup.find(\"ul\", class_=html_class)\n",
    "        if not section:\n",
    "            continue\n",
    "        for item in section.find_all(\"li\"):\n",
    "            spans = item.find_all(\"span\")\n",
    "            if len(spans) > 1:\n",
    "                key = spans[0].get_text(strip=True)\n",
    "                if key == \"Follow player\":\n",
    "                    continue\n",
    "                value = spans[1].get_text(strip=True)\n",
    "                data[key] = value\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "934b7ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 4: Scrape top-N player rows (rank, player, links, 'Tourns') ---\n",
    "\n",
    "def scrape_player_urls(dateweek: str, tournament: str, year: str, n_rows: int = N_ROWS_DEFAULT) -> List[Dict]:\n",
    "    ranking_url = f\"https://www.atptour.com/en/rankings/doubles?RankRange=1-100&Region=all&DateWeek={dateweek}\"\n",
    "    driver = configure_driver()\n",
    "    try:\n",
    "        driver.get(ranking_url)\n",
    "        random_sleep()\n",
    "\n",
    "        # Wait for the table to render\n",
    "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.CSS_SELECTOR, \"tbody\")))\n",
    "\n",
    "        # Parse the HTML table (Rank / Player / Link)\n",
    "        page_source = driver.page_source\n",
    "        players = parse_rankings_table(page_source, n_rows)\n",
    "\n",
    "        # Collect the “Tourns” column using Selenium (it may be dynamic)\n",
    "        tourn_cells = driver.find_elements(By.XPATH, \"//td[contains(@class,'tourns')]\")\n",
    "        # Align with the first n_rows\n",
    "        tourn_texts = [cell.text.strip() for cell in tourn_cells[:len(players)]]\n",
    "\n",
    "        # Merge back and add static info\n",
    "        merged = []\n",
    "        for i, p in enumerate(players):\n",
    "            p = p.copy()\n",
    "            p[\"Tourns\"] = tourn_texts[i] if i < len(tourn_texts) else \"\"\n",
    "            p[\"Tournament\"] = tournament\n",
    "            p[\"Year\"] = year\n",
    "            p[\"DateWeek\"] = dateweek\n",
    "            merged.append(p)\n",
    "\n",
    "        return merged\n",
    "\n",
    "    except (WebDriverException, TimeoutException) as e:\n",
    "        print(f\"[Rankings] Error for {dateweek} {tournament} {year}: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0c7a0fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 5: Scrape a single player profile page ---\n",
    "\n",
    "def scrape_player_profile(profile_link: str) -> Dict:\n",
    "    driver = configure_driver()\n",
    "    try:\n",
    "        for attempt in range(1, RETRIES + 1):\n",
    "            try:\n",
    "                driver.get(profile_link)\n",
    "                # Wait for any profile section to appear\n",
    "                WebDriverWait(driver, 15).until(\n",
    "                    EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"ul.pd_left, ul.pd_right, div.wins, div.titles\"))\n",
    "                )\n",
    "                random_sleep(0.4, 1.2)\n",
    "                soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "                data = {}\n",
    "                data.update(parse_player_profile_overview(soup))\n",
    "                data.update(parse_player_profile_details(soup))\n",
    "                return data\n",
    "\n",
    "            except (TimeoutException, WebDriverException) as e:\n",
    "                print(f\"[Profile] Attempt {attempt}/{RETRIES} failed: {e}\")\n",
    "                random_sleep(1.5, 3.0)\n",
    "\n",
    "        print(f\"[Profile] Failed to scrape after {RETRIES} attempts: {profile_link}\")\n",
    "        return {}\n",
    "\n",
    "    except WebDriverException as e:\n",
    "        print(f\"[Profile] Critical driver error: {e}\")\n",
    "        return {}\n",
    "    finally:\n",
    "        driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2e5d41a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 6: Save results to Excel ---\n",
    "\n",
    "def save_player_info_to_excel(records: List[Dict], dateweek: str, out_dir: str = OUTPUT_DIR) -> str:\n",
    "    if not records:\n",
    "        print(f\"[Save] No data to save for {dateweek}.\")\n",
    "        return \"\"\n",
    "\n",
    "    df = pd.DataFrame(records)\n",
    "    out_path = os.path.join(out_dir, f\"ranking_doubles_{dateweek}.xlsx\")\n",
    "    df.to_excel(out_path, index=False)\n",
    "    print(f\"[Save] {len(df)} rows saved to: {out_path}\")\n",
    "    return out_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "82f795a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Cell 8: Full run over DATEWEEKS_TOURNAMENTS (self-contained, no run_one_week) ---\n",
    "\n",
    "def run_full_schedule(dateweeks_tournaments=DATEWEEKS_TOURNAMENTS, n_rows=N_ROWS_DEFAULT) -> None:\n",
    "    for dateweek, tournament, year in dateweeks_tournaments:\n",
    "        try:\n",
    "            # 1) Rankings page: collect players\n",
    "            players = scrape_player_urls(dateweek, tournament, year, n_rows=n_rows)\n",
    "            print(f\"[Week] {dateweek} {tournament} {year}: found {len(players)} players.\")\n",
    "\n",
    "            # 2) For each player, enrich with profile details\n",
    "            for idx, p in enumerate(players, start=1):\n",
    "                link = p.get(\"Player Profile Link\") or \"\"\n",
    "                if link:\n",
    "                    profile = scrape_player_profile(link)\n",
    "                    p.update(profile)\n",
    "                print(f\"    - ({idx}/{len(players)}) {p.get('Player','?')} processed.\")\n",
    "                random_sleep(0.5, 1.3)\n",
    "\n",
    "            # 3) Save results\n",
    "            if players:\n",
    "                save_player_info_to_excel(players, dateweek, out_dir=OUTPUT_DIR)\n",
    "            else:\n",
    "                print(f\"[Week] No records for {dateweek}. Skipping save.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[Week] Unexpected error for {dateweek} {tournament} {year}: {e}\")\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eefa2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Week] 2018-01-15 Australian Open 2018: found 5 players.\n",
      "    - (1/5) L. Kubot processed.\n",
      "    - (2/5) M. Melo processed.\n",
      "    - (3/5) H. Kontinen processed.\n",
      "    - (4/5) J. Peers processed.\n",
      "    - (5/5) I. Dodig processed.\n",
      "[Save] 5 rows saved to: C:/Users/aldi/Documents/GitHub/tennis-homophily/data/atp\\ranking_doubles_2018-01-15.xlsx\n"
     ]
    }
   ],
   "source": [
    "# --- Cell 9: Kick it off ---\n",
    "\n",
    "# Start with a small test to verify everything runs:\n",
    "test_subset = DATEWEEKS_TOURNAMENTS[:1]\n",
    "run_full_schedule(test_subset, n_rows=5)\n",
    "\n",
    "# Run full list when ready:\n",
    "# run_full_schedule(DATEWEEKS_TOURNAMENTS, n_rows=50)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
