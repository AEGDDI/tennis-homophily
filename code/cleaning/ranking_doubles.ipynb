{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53b4ecc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from getpass import getuser\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9cc99636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23 files.\n",
      "Processed: ranking_doubles_2018-01-15.xlsx\n",
      "Processed: ranking_doubles_2018-05-21.xlsx\n",
      "Processed: ranking_doubles_2018-07-02.xlsx\n",
      "Processed: ranking_doubles_2018-09-24.xlsx\n",
      "Processed: ranking_doubles_2019-01-14.xlsx\n",
      "Processed: ranking_doubles_2019-05-20.xlsx\n",
      "Processed: ranking_doubles_2019-07-01.xlsx\n",
      "Processed: ranking_doubles_2019-08-26.xlsx\n",
      "Processed: ranking_doubles_2020-01-20.xlsx\n",
      "Processed: ranking_doubles_2020-08-31.xlsx\n",
      "Processed: ranking_doubles_2020-09-21.xlsx\n",
      "Processed: ranking_doubles_2021-02-08.xlsx\n",
      "Processed: ranking_doubles_2021-05-24.xlsx\n",
      "Processed: ranking_doubles_2021-06-28.xlsx\n",
      "Processed: ranking_doubles_2021-08-30.xlsx\n",
      "Processed: ranking_doubles_2022-01-17.xlsx\n",
      "Processed: ranking_doubles_2022-05-16.xlsx\n",
      "Processed: ranking_doubles_2022-06-27.xlsx\n",
      "Processed: ranking_doubles_2022-08-22.xlsx\n",
      "Processed: ranking_doubles_2023-01-16.xlsx\n",
      "Processed: ranking_doubles_2023-05-22.xlsx\n",
      "Processed: ranking_doubles_2023-07-03.xlsx\n",
      "Processed: ranking_doubles_2023-08-28.xlsx\n",
      "Rows per file: [99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99, 99]\n",
      "Expected total rows: 2277\n",
      "Actual merged rows: 2277\n",
      "✅ Row count matches! No rows lost or duplicated during merge.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "USER = getuser()\n",
    "INPUT_DIR = f\"C:/Users/{USER}/Documents/GitHub/tennis-homophily/data/atp/\"\n",
    "\n",
    "# Collect all ranking_doubles files\n",
    "files = [\n",
    "    f for f in os.listdir(INPUT_DIR)\n",
    "    if f.startswith(\"ranking_doubles_\") and f.endswith(\".xlsx\")\n",
    "]\n",
    "\n",
    "print(f\"Found {len(files)} files.\")\n",
    "\n",
    "dfs = []\n",
    "for file in sorted(files):\n",
    "    path = os.path.join(INPUT_DIR, file)\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_excel(path)\n",
    "\n",
    "        # Drop unwanted columns if they exist\n",
    "        cols_to_drop = [\"Unnamed: 15\", \"Player Profile Link\"]\n",
    "        df = df.drop(columns=[c for c in cols_to_drop if c in df.columns], errors=\"ignore\")\n",
    "        \n",
    "        # Standardize column names: lower case + replace space with -\n",
    "        df.columns = df.columns.str.lower().str.replace(\" \", \"-\", regex=False)\n",
    "\n",
    "        dfs.append(df)\n",
    "        print(f\"Processed: {file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "\n",
    "# Count rows in each single df\n",
    "rows_per_df = [len(d) for d in dfs]\n",
    "\n",
    "# Total expected rows\n",
    "expected_rows = sum(rows_per_df)\n",
    "\n",
    "print(\"Rows per file:\", rows_per_df)\n",
    "print(\"Expected total rows:\", expected_rows)\n",
    "\n",
    "# Merge\n",
    "merged_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "actual_rows = len(merged_df)\n",
    "\n",
    "print(\"Actual merged rows:\", actual_rows)\n",
    "\n",
    "if actual_rows == expected_rows:\n",
    "    print(\"✅ Row count matches! No rows lost or duplicated during merge.\")\n",
    "else:\n",
    "    print(\"❌ Mismatch! Expected:\", expected_rows, \"but got:\", actual_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469222da",
   "metadata": {},
   "source": [
    "# clear players'names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d5f69482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All columns that represent players\n",
    "player_cols = [col for col in merged_df.columns if col.startswith(\"player\")]\n",
    "\n",
    "def split_player_initial_surname(x):\n",
    "    \"\"\"\n",
    "    Clean a raw player string and return:\n",
    "    (first_initial, surname)\n",
    "\n",
    "    Examples:\n",
    "      'L. Kubot'            -> ('l', 'kubot')\n",
    "      'l.kubot'             -> ('l', 'kubot')\n",
    "      'Bob Bryan'           -> ('b', 'bryan')\n",
    "      '1O. Marach'          -> ('m', 'marach')\n",
    "      '-1R. A. Harrison'    -> ('a', 'harrison')\n",
    "      'Mannarino'           -> ('m', 'mannarino')\n",
    "    \"\"\"\n",
    "    if pd.isna(x):\n",
    "        return (None, None)\n",
    "\n",
    "    s = str(x).lower().strip()\n",
    "\n",
    "    # remove leading junk such as \"-1R.\", \"1O.\", numbers, etc.\n",
    "    s = re.sub(r\"^[^a-z]+\", \"\", s)\n",
    "\n",
    "    # normalize dots to spaces: \"l.kubot\" -> \"l kubot\"\n",
    "    s = s.replace(\".\", \" \")\n",
    "\n",
    "    # collapse multiple spaces\n",
    "    parts = s.split()\n",
    "    if len(parts) == 0:\n",
    "        return (None, None)\n",
    "\n",
    "    if len(parts) == 1:\n",
    "        surname = parts[0]\n",
    "        initial = surname[0]\n",
    "        return (initial, surname)\n",
    "\n",
    "    # more than one token: use first token's first letter + last token as surname\n",
    "    initial = parts[0][0]\n",
    "    surname = parts[-1]\n",
    "    return (initial, surname)\n",
    "\n",
    "\n",
    "# Apply to each player column and create two new columns\n",
    "for col in player_cols:\n",
    "    initials, surnames = zip(*merged_df[col].apply(split_player_initial_surname))\n",
    "    merged_df[f\"{col}_initial\"] = initials\n",
    "    merged_df[f\"{col}_surname\"] = surnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "967f250b",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop(columns=player_cols, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288fe4e4",
   "metadata": {},
   "source": [
    "# split w-l columns in two separate variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "91ae5608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win–loss columns successfully split.\n",
      "Win-ratio columns successfully created.\n"
     ]
    }
   ],
   "source": [
    "# --- Win-ratio from separate W and L columns --- #\n",
    "def wl_to_ratio_from_cols(wins, losses):\n",
    "    \"\"\"\n",
    "    Calcola il win ratio da due colonne separate:\n",
    "        wins   = numero vittorie\n",
    "        losses = numero sconfitte\n",
    "\n",
    "    Restituisce wins / (wins + losses), oppure NaN se:\n",
    "        - wins o losses sono NaN/non numerici\n",
    "        - wins + losses == 0\n",
    "    \"\"\"\n",
    "    wins = pd.to_numeric(wins, errors=\"coerce\")\n",
    "    losses = pd.to_numeric(losses, errors=\"coerce\")\n",
    "\n",
    "    total = wins + losses\n",
    "    ratio = wins / total\n",
    "\n",
    "    ratio[(total <= 0) | total.isna()] = np.nan\n",
    "    return ratio\n",
    "\n",
    "\n",
    "# --- 1. Individua tutte le colonne win-loss --- #\n",
    "# matches: doubles_w-l-ytd, singles_wl_career, ecc.\n",
    "\n",
    "wl_cols = [\n",
    "    col for col in merged_df.columns\n",
    "    if re.search(r\"(?:^|[_-])w[-_]?l(?:[_-]|$)\", col.lower())\n",
    "]\n",
    "\n",
    "\n",
    "def split_wl(value):\n",
    "    \"\"\"\n",
    "    Converte stringhe tipo '24-28' o '435 - 305' in (wins, losses).\n",
    "    \"\"\"\n",
    "    if pd.isna(value):\n",
    "        return (np.nan, np.nan)\n",
    "\n",
    "    match = re.findall(r\"\\d+\", str(value))\n",
    "    if len(match) == 2:\n",
    "        return int(match[0]), int(match[1])\n",
    "    return (np.nan, np.nan)\n",
    "\n",
    "\n",
    "# Manteniamo traccia delle coppie wins/losses create\n",
    "wl_pairs = []\n",
    "\n",
    "for col in wl_cols:\n",
    "    # normalizza nome base: w-l / wl → win_loss\n",
    "    base = re.sub(r\"w[-_]?l\", \"win_loss\", col, flags=re.IGNORECASE)\n",
    "\n",
    "    wins_col = base.replace(\"win_loss\", \"wins\")\n",
    "    losses_col = base.replace(\"win_loss\", \"losses\")\n",
    "\n",
    "    merged_df[wins_col], merged_df[losses_col] = zip(\n",
    "        *merged_df[col].apply(split_wl)\n",
    "    )\n",
    "\n",
    "    wl_pairs.append((wins_col, losses_col))\n",
    "\n",
    "    merged_df.drop(columns=col, inplace=True)\n",
    "\n",
    "print(\"Win–loss columns successfully split.\")\n",
    "\n",
    "\n",
    "# --- 2. Crea le colonne di win ratio --- #\n",
    "\n",
    "for wins_col, losses_col in wl_pairs:\n",
    "    ratio_col = wins_col.replace(\"wins\", \"win_ratio\")\n",
    "\n",
    "    merged_df[ratio_col] = wl_to_ratio_from_cols(\n",
    "        merged_df[wins_col],\n",
    "        merged_df[losses_col]\n",
    "    )\n",
    "\n",
    "print(\"Win-ratio columns successfully created.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ea7252",
   "metadata": {},
   "source": [
    "# extract the missing values of dob from age "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "70229401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOB values updated where possible.\n"
     ]
    }
   ],
   "source": [
    "def extract_dob_from_age(age_value):\n",
    "    \"\"\"\n",
    "    Extracts the date inside parentheses in the format YYYY/MM/DD.\n",
    "    Example: '42 (1983/09/23)' → '1983/09/23'\n",
    "    \"\"\"\n",
    "    if pd.isna(age_value):\n",
    "        return None\n",
    "\n",
    "    age_str = str(age_value)\n",
    "\n",
    "    # Look for a date inside parentheses\n",
    "    match = re.search(r\"\\((\\d{4}/\\d{2}/\\d{2})\\)\", age_str)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n",
    "\n",
    "# Apply only to rows where dob is missing\n",
    "missing_dob_mask = merged_df[\"dob\"].isna()\n",
    "\n",
    "merged_df.loc[missing_dob_mask, \"dob\"] = merged_df.loc[missing_dob_mask, \"age\"].apply(\n",
    "    extract_dob_from_age\n",
    ")\n",
    "\n",
    "print(\"DOB values updated where possible.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4604d6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=[\"age\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f62fa5a",
   "metadata": {},
   "source": [
    "# weight and height split in two different measures \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12f02324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight and height cleaned successfully.\n"
     ]
    }
   ],
   "source": [
    "# ---------- WEIGHT CLEANING ----------\n",
    "def extract_weight_lbs(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    match = re.search(r\"(\\d+)\\s*lbs\", str(x).lower())\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def extract_weight_kg(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    match = re.search(r\"\\((\\d+)\\s*kg\\)\", str(x).lower())\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "if \"weight\" in merged_df.columns:\n",
    "    merged_df[\"weight-lbs\"] = merged_df[\"weight\"].apply(extract_weight_lbs)\n",
    "    merged_df[\"weight-kg\"]  = merged_df[\"weight\"].apply(extract_weight_kg)\n",
    "    merged_df.drop(columns=[\"weight\"], inplace=True)\n",
    "\n",
    "\n",
    "# ---------- HEIGHT CLEANING ----------\n",
    "def extract_height_ft(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    match = re.search(r\"(\\d+)'\\s*(\\d+)\\\"\", str(x).lower())\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "def extract_height_in(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    match = re.search(r\"(\\d+)'\\s*(\\d+)\\\"\", str(x).lower())\n",
    "    return int(match.group(2)) if match else None\n",
    "\n",
    "def extract_height_cm(x):\n",
    "    if pd.isna(x):\n",
    "        return None\n",
    "    match = re.search(r\"\\((\\d+)\\s*cm\\)\", str(x).lower())\n",
    "    return int(match.group(1)) if match else None\n",
    "\n",
    "if \"height\" in merged_df.columns:\n",
    "    merged_df[\"height-ft\"] = merged_df[\"height\"].apply(extract_height_ft)\n",
    "    merged_df[\"height-in\"] = merged_df[\"height\"].apply(extract_height_in)\n",
    "    merged_df[\"height-cm\"] = merged_df[\"height\"].apply(extract_height_cm)\n",
    "    merged_df.drop(columns=[\"height\"], inplace=True)\n",
    "\n",
    "print(\"Weight and height cleaned successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e00507",
   "metadata": {},
   "source": [
    "# variable experience = year - turned pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d82cf3d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['rank', 'tourns', 'tournament', 'year', 'dateweek',\n",
       "       'doubles_titles-ytd', 'doubles_titles-career', 'dob', 'turned-pro',\n",
       "       'country', 'birthplace', 'plays', 'coach', 'singles_career_high_rank',\n",
       "       'singles_date_career_high_rank', 'singles_titles_career',\n",
       "       'player_initial', 'player_surname', 'doubles_wins-ytd',\n",
       "       'doubles_losses-ytd', 'doubles_wins-career', 'doubles_losses-career',\n",
       "       'singles_wins_career', 'singles_losses_career', 'doubles_win_ratio-ytd',\n",
       "       'doubles_win_ratio-career', 'singles_win_ratio_career', 'weight-lbs',\n",
       "       'weight-kg', 'height-ft', 'height-in', 'height-cm'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4c2bd4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create experience variable only if both columns exist\n",
    "if \"year\" in merged_df.columns and \"turned-pro\" in merged_df.columns:\n",
    "    merged_df[\"experience_double\"] = merged_df[\"year\"] - merged_df[\"turned-pro\"]\n",
    "else:\n",
    "    print(\"Column 'year' or 'turned-pro' is missing.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f4b79",
   "metadata": {},
   "source": [
    "# variables single rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6378d920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Parse dates (coerce invalid to NaT)\n",
    "merged_df[\"dateweek_dt\"] = pd.to_datetime(merged_df[\"dateweek\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "merged_df[\"singles_rank_date_dt\"] = pd.to_datetime(\n",
    "    merged_df[\"singles_date_career_high_rank\"], format=\"%Y.%m.%d\", errors=\"coerce\"\n",
    ")\n",
    "merged_df[\"dob_dt\"] = pd.to_datetime(merged_df[\"dob\"], format=\"%Y/%m/%d\", errors=\"coerce\")\n",
    "\n",
    "# 2) Difference in days: dateweek - rank date\n",
    "merged_df[\"days_since_single_career_high_rank\"] = (\n",
    "    merged_df[\"dateweek_dt\"] - merged_df[\"singles_rank_date_dt\"]\n",
    ").dt.days\n",
    "\n",
    "# 3) Age when they reached career-high rank (in years)\n",
    "merged_df[\"age_at_single_career_high_rank_years\"] = (\n",
    "    (merged_df[\"singles_rank_date_dt\"] - merged_df[\"dob_dt\"]).dt.days / 365.25\n",
    ")\n",
    "\n",
    "# Optional: also store as whole days (more “exact”)\n",
    "merged_df[\"age_at_career_high_rank_days\"] = (\n",
    "    merged_df[\"singles_rank_date_dt\"] - merged_df[\"dob_dt\"]\n",
    ").dt.days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5cdb8cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only meaningful if peak is in the past (>=0 days) and peak rank <= 100\n",
    "is_top100_peak = merged_df[\"singles_career_high_rank\"].le(100)\n",
    "peak_in_past   = merged_df[\"days_since_single_career_high_rank\"].ge(0)\n",
    "\n",
    "d = merged_df[\"days_since_single_career_high_rank\"]\n",
    "\n",
    "merged_df[\"top100_within_1y\"] = (\n",
    "    is_top100_peak & peak_in_past & (d <= 365)\n",
    ").astype(int)\n",
    "\n",
    "merged_df[\"top100_within_5y\"] = (\n",
    "    is_top100_peak & peak_in_past & (d > 365) & (d <= 5 * 365)\n",
    ").astype(int)\n",
    "\n",
    "merged_df[\"top100_within_10y\"] = (\n",
    "    is_top100_peak & peak_in_past & (d > 5 * 365) & (d <= 10 * 365)\n",
    ").astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4d04d873",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make sure ranks are numeric\n",
    "merged_df[\"rank_num\"] = (\n",
    "    merged_df[\"rank\"]\n",
    "    .astype(str)\n",
    "    .str.extract(r\"(\\d+)\", expand=False)\n",
    "    .astype(float)\n",
    ")\n",
    "\n",
    "merged_df[\"singles_career_high_rank_num\"] = pd.to_numeric(\n",
    "    merged_df[\"singles_career_high_rank\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "base_gap = merged_df[\"rank_num\"] - merged_df[\"singles_career_high_rank_num\"]\n",
    "\n",
    "d = merged_df[\"days_since_single_career_high_rank\"]\n",
    "is_top100_peak = merged_df[\"singles_career_high_rank_num\"].le(100)\n",
    "peak_in_past   = d.ge(0)\n",
    "\n",
    "cond_1y  = is_top100_peak & peak_in_past & (d <= 365)\n",
    "cond_5y  = is_top100_peak & peak_in_past & (d > 365) & (d <= 5 * 365)\n",
    "cond_10y = is_top100_peak & peak_in_past & (d > 5 * 365) & (d <= 10 * 365)\n",
    "\n",
    "merged_df[\"gap_1y\"]  = np.where(cond_1y,  base_gap, np.nan)\n",
    "merged_df[\"gap_5y\"]  = np.where(cond_5y,  base_gap, np.nan)\n",
    "merged_df[\"gap_10y\"] = np.where(cond_10y, base_gap, np.nan)\n",
    "\n",
    "merged_df[\"abs_gap_1y\"]  = merged_df[\"gap_1y\"].abs()\n",
    "merged_df[\"abs_gap_5y\"]  = merged_df[\"gap_5y\"].abs()\n",
    "merged_df[\"abs_gap_10y\"] = merged_df[\"gap_10y\"].abs()\n",
    "\n",
    "merged_df[\"single_specialist\"] = (\n",
    "    merged_df[\"singles_career_high_rank_num\"].le(100) &\n",
    "    base_gap.gt(30)\n",
    ").astype(int)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61945b67",
   "metadata": {},
   "source": [
    "# birthplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cba336b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_birthplace(x):\n",
    "    if pd.isna(x):\n",
    "        return (None, None)\n",
    "    \n",
    "    parts = str(x).split(\",\")\n",
    "    \n",
    "    if len(parts) == 2:\n",
    "        city = parts[0].strip()\n",
    "        country = parts[1].strip()\n",
    "        return city, country\n",
    "    else:\n",
    "        # If the format is not city, country → return entire string as city\n",
    "        return str(x).strip(), None\n",
    "\n",
    "# Apply split\n",
    "merged_df[\"birthplace-city\"], merged_df[\"birthplace-country\"] = zip(\n",
    "    *merged_df[\"birthplace\"].apply(split_birthplace)\n",
    ")\n",
    "\n",
    "# Optional: drop the original column\n",
    "merged_df.drop(columns=[\"birthplace\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2473c9",
   "metadata": {},
   "source": [
    "# hand and backhand "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "11efa520",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_plays(x):\n",
    "    if pd.isna(x):\n",
    "        return (None, None)\n",
    "    \n",
    "    parts = str(x).split(\",\")\n",
    "    \n",
    "    if len(parts) == 2:\n",
    "        hand = parts[0].strip()\n",
    "        backhand = parts[1].strip()\n",
    "        return hand, backhand\n",
    "    else:\n",
    "        # If format unexpected, store everything in \"hand\"\n",
    "        return str(x).strip(), None\n",
    "\n",
    "merged_df[\"hand\"], merged_df[\"backhand\"] = zip(\n",
    "    *merged_df[\"plays\"].apply(split_plays)\n",
    ")\n",
    "\n",
    "# Optional: drop the original column\n",
    "merged_df.drop(columns=[\"plays\"], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2224e2",
   "metadata": {},
   "source": [
    "# colonial legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "86d84632",
   "metadata": {},
   "outputs": [],
   "source": [
    "colonial_map = {\n",
    "    # Spanish legacy\n",
    "    \"Spain\": \"Spanish\",\n",
    "    \"Argentina\": \"Spanish\",\n",
    "    \"Chile\": \"Spanish\",\n",
    "    \"Peru\": \"Spanish\",\n",
    "    \"Mexico\": \"Spanish\",\n",
    "    \"Colombia\": \"Spanish\",\n",
    "    \"Uruguay\": \"Spanish\",\n",
    "    \"Venezuela\": \"Spanish\",\n",
    "    \"Ecuador\": \"Spanish\",\n",
    "    \"Bolivia\": \"Spanish\",\n",
    "    \"Paraguay\": \"Spanish\",\n",
    "    \"Costa Rica\": \"Spanish\",\n",
    "    \"Panama\": \"Spanish\",\n",
    "    \"El Salvador\": \"Spanish\",\n",
    "\n",
    "    # Portuguese legacy\n",
    "    \"Portugal\": \"Portuguese\",\n",
    "    \"Brazil\": \"Portuguese\",\n",
    "\n",
    "    # Dutch legacy\n",
    "    \"Netherlands\": \"Dutch\",\n",
    "    \"Indonesia\": \"Dutch\",\n",
    "\n",
    "    # French legacy\n",
    "    \"France\": \"French\",\n",
    "    \"Senegal\": \"French\",\n",
    "    \"Ivory Coast\": \"French\",\n",
    "    \"Côte d'Ivoire\": \"French\",\n",
    "    \"Mali\": \"French\",\n",
    "    \"Cameroon\": \"French\",\n",
    "    \"Morocco\": \"French\",\n",
    "    \"Tunisia\": \"French\",\n",
    "    \"Monaco\": \"French\",\n",
    "\n",
    "    # British legacy\n",
    "    \"United Kingdom\": \"British\",\n",
    "    \"Great Britain\": \"British\",\n",
    "    \"UK\": \"British\",\n",
    "    \"England\": \"British\",\n",
    "    \"Australia\": \"British\",\n",
    "    \"United States\": \"British\",\n",
    "    \"USA\": \"British\",\n",
    "    \"Canada\": \"British\",\n",
    "    \"India\": \"British\",\n",
    "    \"South Africa\": \"British\",\n",
    "    \"New Zealand\": \"British\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "99e52993",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"colonial_legacy\"] = (\n",
    "    merged_df[\"country\"]\n",
    "    .map(colonial_map)\n",
    "    .fillna(\"None\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4551f951",
   "metadata": {},
   "source": [
    "# federal legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fdb2e22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "federal_map = {\n",
    "    # -------------------------\n",
    "    # Yugoslavia (SFRY)\n",
    "    # -------------------------\n",
    "    \"Croatia\": \"Yugoslavia\",\n",
    "    \"Serbia\": \"Yugoslavia\",\n",
    "    \"Slovenia\": \"Yugoslavia\",\n",
    "    \"Bosnia and Herzegovina\": \"Yugoslavia\",\n",
    "    \"Bosnia & Herzegovina\": \"Yugoslavia\",\n",
    "    \"Bosnia\": \"Yugoslavia\",\n",
    "    \"Montenegro\": \"Yugoslavia\",\n",
    "    \"North Macedonia\": \"Yugoslavia\",\n",
    "    \"Macedonia\": \"Yugoslavia\",\n",
    "\n",
    "    # -------------------------\n",
    "    # Czechoslovakia\n",
    "    # -------------------------\n",
    "    \"Czech Republic\": \"Czechoslovakia\",\n",
    "    \"Czechia\": \"Czechoslovakia\",\n",
    "    \"Slovakia\": \"Czechoslovakia\",\n",
    "    \"Slovak Republic\": \"Czechoslovakia\",\n",
    "\n",
    "    # -------------------------\n",
    "    # Soviet Union (USSR)\n",
    "    # -------------------------\n",
    "    \"Russia\": \"Soviet Union\",\n",
    "    \"Russian Federation\": \"Soviet Union\",\n",
    "    \"Ukraine\": \"Soviet Union\",\n",
    "    \"Belarus\": \"Soviet Union\",\n",
    "    \"Moldova\": \"Soviet Union\",\n",
    "\n",
    "    \"Estonia\": \"Soviet Union\",\n",
    "    \"Latvia\": \"Soviet Union\",\n",
    "    \"Lithuania\": \"Soviet Union\",\n",
    "\n",
    "    \"Georgia\": \"Soviet Union\",\n",
    "    \"Armenia\": \"Soviet Union\",\n",
    "    \"Azerbaijan\": \"Soviet Union\",\n",
    "\n",
    "    \"Kazakhstan\": \"Soviet Union\",\n",
    "    \"Uzbekistan\": \"Soviet Union\",\n",
    "    \"Turkmenistan\": \"Soviet Union\",\n",
    "    \"Kyrgyzstan\": \"Soviet Union\",\n",
    "    \"Tajikistan\": \"Soviet Union\",\n",
    "\n",
    "    # -------------------------\n",
    "    # Nordic countries\n",
    "    # -------------------------\n",
    "    \"Norway\": \"Nordic\",\n",
    "    \"Sweden\": \"Nordic\",\n",
    "    \"Denmark\": \"Nordic\",\n",
    "    \"Finland\": \"Nordic\",\n",
    "    \"Iceland\": \"Nordic\",\n",
    "\n",
    "    # -------------------------\n",
    "    # Germanic countries (your definition)\n",
    "    # -------------------------\n",
    "    \"Germany\": \"Germanic\",\n",
    "    \"Austria\": \"Germanic\",\n",
    "    \"Netherlands\": \"Germanic\",\n",
    "    \"Belgium\": \"Germanic\",  \n",
    "\n",
    "    # -------------------------\n",
    "    # Eastern Europe (non-federal)\n",
    "    # -------------------------\n",
    "    \"Poland\": \"Eastern Europe\",\n",
    "    \"Hungary\": \"Eastern Europe\",\n",
    "    \"Romania\": \"Eastern Europe\",\n",
    "    \"Bulgaria\": \"Eastern Europe\",\n",
    "    \"Greece\": \"Eastern Europe\" \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e464cda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"federal_legacy\"] = (\n",
    "    merged_df[\"country\"]\n",
    "    .map(federal_map)\n",
    "    .fillna(\"None\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04f9e9",
   "metadata": {},
   "source": [
    "# Country ISO3 mapping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c888d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Poland', 'Brazil', 'Finland', 'Australia', 'Croatia', 'France',\n",
       "       'Netherlands', 'Romania', 'Great Britain', 'United States',\n",
       "       'Spain', 'New Zealand', 'Austria', 'India', 'Uruguay', 'Colombia',\n",
       "       'South Africa', 'Mexico', 'Chile', 'Pakistan', 'Argentina',\n",
       "       'Serbia', nan, 'Germany', 'Czechia', 'Canada', 'Philippines',\n",
       "       'Sweden', 'Japan', 'Monaco', 'Israel', 'Italy', 'Thailand',\n",
       "       'El Salvador', 'Belgium', 'Portugal', 'Slovak Republic',\n",
       "       'Indonesia', 'Ukraine', 'Moldova', 'Denmark', 'Chinese Taipei',\n",
       "       'China', 'Greece', 'Tunisia', 'Venezuela', 'Ecuador', 'Kazakhstan',\n",
       "       'Bosnia-Herzegovina', 'Jamaica'], dtype=object)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_df[\"country\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "40d4cb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "country_to_iso3 = {\n",
    "    'Poland': 'POL',\n",
    "    'Brazil': 'BRA',\n",
    "    'Finland': 'FIN',\n",
    "    'Australia': 'AUS',\n",
    "    'Croatia': 'HRV',\n",
    "    'France': 'FRA',\n",
    "    'Netherlands': 'NLD',\n",
    "    'Romania': 'ROU',\n",
    "    'Great Britain': 'GBR',\n",
    "    'United States': 'USA',\n",
    "    'Spain': 'ESP',\n",
    "    'New Zealand': 'NZL',\n",
    "    'Austria': 'AUT',\n",
    "    'India': 'IND',\n",
    "    'Uruguay': 'URY',\n",
    "    'Colombia': 'COL',\n",
    "    'South Africa': 'ZAF',\n",
    "    'Mexico': 'MEX',\n",
    "    'Chile': 'CHL',\n",
    "    'Pakistan': 'PAK',\n",
    "    'Argentina': 'ARG',\n",
    "    'Serbia': 'SRB',\n",
    "    'Germany': 'DEU',\n",
    "    'Czechia': 'CZE',\n",
    "    'Canada': 'CAN',\n",
    "    'Philippines': 'PHL',\n",
    "    'Sweden': 'SWE',\n",
    "    'Japan': 'JPN',\n",
    "    'Monaco': 'MCO',\n",
    "    'Israel': 'ISR',\n",
    "    'Italy': 'ITA',\n",
    "    'Thailand': 'THA',\n",
    "    'El Salvador': 'SLV',\n",
    "    'Belgium': 'BEL',\n",
    "    'Portugal': 'PRT',\n",
    "    'Slovak Republic': 'SVK',\n",
    "    'Indonesia': 'IDN',\n",
    "    'Ukraine': 'UKR',\n",
    "    'Moldova': 'MDA',\n",
    "    'Denmark': 'DNK',\n",
    "    'Chinese Taipei': 'TWN',\n",
    "    'China': 'CHN',\n",
    "    'Greece': 'GRC',\n",
    "    'Tunisia': 'TUN',\n",
    "    'Venezuela': 'VEN',\n",
    "    'Ecuador': 'ECU',\n",
    "    'Kazakhstan': 'KAZ',\n",
    "    'Bosnia-Herzegovina': 'BIH',\n",
    "    'Jamaica': 'JAM'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "36107eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"country_std\"] = merged_df[\"country\"].astype(str).str.strip()\n",
    "merged_df[\"iso3\"] = merged_df[\"country_std\"].map(country_to_iso3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "830f412d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan], dtype=object)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df[merged_df[\"iso3\"].isna()][\"country\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff25c47",
   "metadata": {},
   "source": [
    "# export output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "65de769f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged file saved as: C:/Users/ALESSANDRO/Documents/GitHub/tennis-homophily/data/atp/men_rankings.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Save output\n",
    "output_path = os.path.join(INPUT_DIR, \"men_rankings.xlsx\")\n",
    "merged_df.to_excel(output_path, index=False)\n",
    "\n",
    "print(f\"\\nMerged file saved as: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
